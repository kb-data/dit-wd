---
title: 'Working with Data Assignment 1'
author: 'Kate Byrne'
date: '31 October 2018'
output: html_document
---
### Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include=TRUE, warning=FALSE, message=FALSE)
options(digits=2)
```

Required R libraries are below
```{r, message=FALSE}
library(devtools)
library(readr)
library(ggplot2)
library(ggmap)
devtools::install_github('hadley/ggplot2')
devtools::install_github('dkahle/ggmap')
library(httr)
library(plyr)
library(jsonlite)
library(dplyr)
library(readODS)
library(knitr)
library(kableExtra)
library(hms)
library(lubridate)
library(tidyr)
library(stringi)
library(scales)
library(grid)
library(gridExtra)
```


##Task 1: Quarterly Economic Indicators
###1.1 Dublin Employment Trends
Figure 1 below is the plot of quarterly Dublin Employment Trends for the period 2006 - 2016.

```{r include=TRUE, results='hide', fig.height=5, fig.width=12}
# Loading reading in and plotting the Dublin employment trends data
employ.read <- read_delim('dublin employment trends.txt', ':')
qplot(data = employ.read , x = Time , y = Employment, geom = 'line', xlab = 'Quarterly Figures', ylab = 'Trend', main = 'Figure 1: Dublin Employment Trends Per Sector: 2006 - 2016') + facet_grid ( .~ Sector) 
```

As the specific dates are hard to read off Figure 1, I've created a table below of the dates of the minimum and maximum employment values for each sector. Finance is at its maximum value for 3 separate quarters.

```{r}
# Creating a table to read the dates of minimum and maximum employment rates 
min.employ <- employ.read %>% group_by(Sector) %>% filter(., Employment == min(Employment))
max.employ <- employ.read %>% group_by(Sector) %>% filter(., Employment == max(Employment))
employ.extreme <- left_join(min.employ, max.employ, by = 'Sector', suffix = c('.min', '.max'))[c('Sector', 'QuarterYear.min', 'Employment.min', 'Time.min', 'QuarterYear.max', 'Employment.max', 'Time.max')]
employ.extreme <- employ.extreme %>% mutate(Employment.diff = Employment.max-Employment.min)
colnames(employ.extreme) <- c('Sector', 'Min Quarter', 'Min Employment Rate', 'Min Time Period', 'Max Quarter', 'Max Employment Rate', 'Max Time Period', 'Min Max Employment Rate Diff')

kable(employ.extreme, digits = 2) %>% kable_styling(bootstrap_options = "striped", full_width = T)
```

From Figure 1 above, and the table of maximum and minimum values, it appears that construction, finance, retail and transport all largely follow the same pattern of peaking in the earlier time period (construction, finance and retail; late 2007/early 2008, transport; late 2010) and then decreasing. 

There is a sharp decrease in both construction and retail, with a less sharp drop in both finance and transport. This would be influenced by the nature of these these industries. In both retail and construction, people are employed to directly provide a product for sale, if sales are down, staff are let go. As many staff in these industries can also be on insecure contracts, this can happen quite quickly. While the volumes of business and profits generated by both the finance and transport sectors was affected by the economic downturn (less people in work would mean less people getting peak time transport, less mortgages and loans were processed in the financial sector), a lot of their services are required regardless of economic factors (regular transportation, maintaining accounts and processing transactions), so while profits may have been down, the number of people employed could not decrease as quickly. Also, people in these industries can have more secure employment contracts so lay-offs might not happen as quickly. 

Of these industries, some either appear to recover fully (transport) or partially (finance, construction). However, retail seems to be continuing an overall downward trajectory. While all sectors would have been affected by the downturn in the economy, there was a 5-fold increase in online shopping in Ireland between 2007 and 2017 [1], which would affect the recovery of high street retail. Also, since many of the large online retailers are not based in Ireland (e.g. ASOS and Amazon are UK based), the shift to online shopping would further reduce the numbers of people employed in retail in Dublin and Ireland. 

Employment in IT have been increasing steadily since the end of 2010. This has been a growing industry in Ireland, and specifically Dublin due to the arrival of several international Tech companies such as AirBnb (2013) and Facebook (2008). 

As the number of tourists coming to Ireland and Dublin has increased in recent years [2], you would expect that the number of people employed in the tourism sector has increased. Since the recession between 2007 and 2009 was a global recession, you would expect the number of tourists coming to Ireland in this period would have been impacted. 

As employment in the scientific sector largely depends on government and industry sponsorship and grants which are often for a fixed period of time such as 3-5 years, we can see why the time period immediately after the recession had the lowest rates, if less new grants were awarded during the recession period, this would impact the number of people employed in the following years.


###1.2 Dublin Property Trends

```{r}
# Reading in Dublin property trends and plotting
prop.read <- read_tsv('dublin property trends.txt')
qplot(data = prop.read , x = Time , y = Trend, geom = 'line', colour = Category, xlab = 'Time', ylab = 'Trend', main = 'Figure 2:Dublin Property Trends: 2007 - 2016')
```

```{r}
# Investivating the time periods where rent outstrips buy and v.v.
prop.split <- split(prop.read, prop.read$Category)
prop.apt.rent <- select(prop.split[[1]], Time, Year, Trend, Category)
prop.house.price <- select(prop.split[[2]], Time, Year, Trend, Category)
prop.house.rent <- select(prop.split[[3]], Time, Year, Trend, Category)
prop.house.built <- select(prop.split[[4]], Time, Year, Trend, Category)

prop.cols <- prop.apt.rent %>% left_join(prop.house.price, by = c('Time', 'Year'), suffix = c('.apt.rent', '.house.price')) %>% left_join( prop.house.built, by = c('Time', 'Year'), suffix = c('.', '.house.built')) %>% left_join(., prop.house.rent, by = c('Time', 'Year'), suffix = c('.', '.house.rent')) 

prop.filter.buy <- filter(prop.cols, (Trend.house.price > Trend.house.rent & Trend.house.price > Trend.apt.rent & Trend. > Trend.house.rent & Trend. > Trend.apt.rent))
prop.filter.rent <- filter(prop.cols, (Trend.house.price < Trend.house.rent & Trend.house.price < Trend.apt.rent & Trend. < Trend.house.rent & Trend. < Trend.apt.rent))
```

The timeperiod in this dataset is from `r unique(filter(prop.read, Time == min(Time))$Year)` to `r unique(filter(prop.read, Time == max(Time))$Year)`. Over this time, the balance of rent vs buy has changed in Dublin. For the time period `r filter(prop.filter.buy, Time == min(Time))$Year` to `r filter(prop.filter.buy, Time == max(Time))$Year`, both house prices and houses built outstripped both houses and apartments rented. For the time period `r filter(prop.filter.rent, Time == min(Time))$Year` to `r filter(prop.filter.rent, Time == max(Time))$Year`, the situation is reversed and both house and apartment rental outstripped house prices and house buying.

Comparing this data to the construction employment figures in the question 1.1, we can see that the number of houses built and house prices behave in a similar pattern to the construction employment figures. The highest rates for house prices, houses built and construction jobs are at Q3 2007, Q3 2007 and Q4 2007 respectively. It should be noted that this is at the start of the dataset for house prices and houses being built so there may have been higher rates before this. The construction employment data started in Q1 2006 so we can say that Q4 2007 is the local maximum rate. Similarly, the lowest rates for house prices, houses built and construction jobs are Q3 2012, Q2 2012 and Q1 2013 respectively.

```{r}
# creating a table to compare house and construction data
min.prop <- prop.read %>% group_by(Category) %>% filter(., Trend == min(Trend))
max.prop <- prop.read %>% group_by(Category) %>% filter(., Trend == max(Trend))
prop.extreme <- left_join(min.prop, max.prop, by = 'Category', suffix = c('.min', '.max'))[c('Category', 'Year.min', 'Trend.min', 'Time.min', 'Year.max', 'Trend.max', 'Time.max')]
prop.extreme <- prop.extreme %>% mutate(Trend.diff = Trend.max-Trend.min)
employ.extreme.con <- employ.extreme %>% filter(., Sector == 'Construction') %>% setNames(c('Category', 'Year.min', 'Trend.min', 'Time.min', 'Year.max',  'Trend.max', 'Time.max', 'Trend.diff'))
kable(bind_rows(prop.extreme, employ.extreme.con), digits = 2) %>% kable_styling(bootstrap_options = "striped", full_width = T)
```


## Task 2: Real Time Bike Info

### 2.1 Summarise information available

```{r include=FALSE}
# Getting list of available cities
stations <- GET('https://api.jcdecaux.com/vls/v1/stations?apiKey=174980a157364085f718726daecb380b7aeaf5b4')
http_type(stations)
stations.df <- jsonlite::fromJSON(content(stations, 'text'), simplifyVector = TRUE)
#stations.df %>% count(contract_name) %>% arrange(desc(n))
```

As Dublin has quite a high number of stations and I'm most familiar with Dublin so I'll choose that.

```{r include=FALSE}
#Pulling in the Dublin bikes data
bike <- GET('https://api.jcdecaux.com/vls/v1/stations?contract=Dublin&apiKey=174980a157364085f718726daecb380b7aeaf5b4')
http_type(bike)
bike.df <- jsonlite::fromJSON(content(bike, 'text'), simplifyVector = TRUE)
```

```{r}
# Creating some aggregated dataframes for brevity in inline code
bike.df.match <- bike.df[tolower(bike.df$name) != tolower(bike.df$address), ]
bike.df.clean <- bike.df %>% select(number, address, banking, bike_stands, available_bikes, available_bike_stands, status)
#is.na(bike.df.clean)
avail.bikes <- rbind(bike.df.clean %>% select(address, Num_bikes=available_bike_stands) %>% mutate(Bike_label = 'Available Bike Stands'), bike.df.clean %>% select(address, Num_bikes=available_bikes) %>% mutate(Bike_label = 'Available Bikes'))
```

At the time that the data was obtained through the API: `r Sys.time()`, all `r nrow(bike.df)`, all Dublin bike stands were `r tolower(count(bike.df, status)[1])`. The bike stands are numbered `r min(bike.df$number)` to `r max(bike.df$number)`, there is no bike stand number 20. 

People generally access Dublin bikes through a subscription service. For 25 euro per year, you get a card that allows you unlimited bike rides of 30 minutes or less. It is possible to buy a 3 day pass with a credit card but this isn't available at the majority of stations. There are payment facilities available at `r count(bike.df, banking) %>% filter(banking==TRUE) %>% select(n)` of the `r nrow(bike.df)` bike stations.

There are `r ncol(bike.df)` columns in the dataset but several are redundant. As we used contract name to chose the data from Dublin, this column doesn't give us any additional information. The name and address columns are identical in most cases. There are 3 cases where they differ but these are either due to spelling or punctuation differences, or a different description of where the station is located, name: `r bike.df.match[bike.df.match$number==50,]$name`, address: `r bike.df.match[bike.df.match$number==50,]$address` and name: `r bike.df.match[bike.df.match$number==58,]$name`, address: `r bike.df.match[bike.df.match$number==58,]$address`. Bike stand number 5 is at an intersection and the name, `r bike.df.match[bike.df.match$number==5,]$name` and address: `r bike.df.match[bike.df.match$number==5,]$address` are each road in the intersection. Therefore for our purposes, we will just use the address field as it's more readable. The bonus field also doesn't appear to add any information, it is `r tolower(bike.df$bonus[1])` in every case. The latitude and longitude of each station is also provided. Aside from the addition of some unneeded fields, the dataset is complete, there are no NAs in fields of interest.

The number of bike stands per station vary between`r min(bike.df$bike_stands)` and `r max(bike.df$bike_stands)`, with an average of `r mean(bike.df$bike_stands)`. At this point in time, there are `r bike.df.clean %>% filter(available_bikes==0) %>% nrow()` stations with no available bikes. There are `r bike.df.clean %>% filter(available_bike_stands==0) %>% nrow()` stations with no free bike stands, so someone would be unable to return a bike they had taken out. There is an average of `r bike.df.clean %>% summarise(mean(available_bikes))` available bikes and `r bike.df.clean %>% summarise(mean(available_bike_stands))` available bike stands per station. The number of available bikes and stands for each station can be seen in Figure 3 below. As the total number of bike stands is the sum of the available bike and available stands, the size of the station can also be seen in the figure below.

```{r fig.height=14, fig.width=12}
ggplot(avail.bikes, aes(fill=Bike_label, y=Num_bikes, x=reorder(address, Num_bikes))) + geom_bar(stat="identity") +
coord_flip() + labs(x = 'Stations', y = 'Number of available bikes or stands', title='Figure 3: Availabiliy of bikes or stands at Dublin bike stations')
```


### 2.2 Plot Dublin Bikes Data

If someone quickly needed to get a Dublin bike, but didn't have a Dublin bike card already set up, they would need a bike station with payment facilities in order to buy a 3 day pass. Below is a plot of the number of available bikes by station and whether the bike station takes payment.

```{r , fig.height=14, fig.width=12}
qplot(available_bikes, reorder(address, bike_stands), data = bike.df , colour = banking, size=I(3)) + labs(x = 'Available Bikes', y = 'Stations', title='Figure 4: Availabiliy of bikes and banking facilities at Dublin bike stations')
```

## Task 3 Dublin Bus
### 3.1 Shaping and describing the data

```{r}
unzip('DublinBus.zip')
trips.df <- read_csv('googletransitdublinbusp20130315-1546\\trips.txt')
routes.df <- read_csv('googletransitdublinbusp20130315-1546\\routes.txt')
calendar.df <- read_csv('googletransitdublinbusp20130315-1546\\calendar.txt')
calendar_dates.df <- read_csv('googletransitdublinbusp20130315-1546\\calendar_dates.txt')
shapes.df <- read_csv('googletransitdublinbusp20130315-1546\\shapes.txt')
stops.df <- read_csv('googletransitdublinbusp20130315-1546\\stops.txt')
transfers.df <- read_csv('googletransitdublinbusp20130315-1546\\transfers.txt')
agency.df <- read_csv('googletransitdublinbusp20130315-1546\\agency.txt')
stop_times.df <- read_csv('googletransitdublinbusp20130315-1546\\stop_times.txt') 
```

There are `r length(unique(routes.df$route_id))` unique routes in this dataset, all of which are route type `r unique(routes.df$route_type)`, 'Bus. Used for short- and long-distance bus routes.' [1]. There was a total of `r length(unique(trips.df$trip_id))` different possible trips. The number of daily journeys for each route, split out by service type (weekdays, Saturdays and Sunday/Bank Holiday) are shown in Figure 5 below.

```{r fig.width=14, fig.height=10}
route.num.trip <- left_join(trips.df, routes.df, by = 'route_id') %>% mutate(Service=case_when(service_id == 2 ~ 'Sunday/Bank Holiday', service_id == 1 ~ 'Weekday',  service_id == 3 ~ 'Saturday')) %>% select(Service, route_short_name, trip_id) %>% count(., Service, route_short_name)

ggplot(data=route.num.trip, aes(x=reorder(route_short_name, -n), y=n)) + facet_grid(rows=vars(Service), scale = 'fixed') + geom_histogram(stat = 'identity', fill='darkorchid3', colour='black', alpha=0.7) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = 'Routes', y = 'Number of Trips', title='Figure 5: Number of journeys on each route by service type')
```


```{r}
#common join used at multiple points in the code
bus.join <- left_join(routes.df, trips.df, by='route_id') %>% left_join(., stop_times.df, by='trip_id') %>% left_join(., stops.df, by='stop_id') %>% mutate(Service=case_when(service_id == 2 ~ 'Sunday/Bank Holiday', service_id == 1 ~ 'Weekday',  service_id == 3 ~ 'Saturday'))

# base df that most and least frequent routes and mean length is based on
bus.freq <- bus.join %>% group_by(route_short_name, service_id,direction_id) %>% filter(., stop_sequence==1) %>% select(., route_short_name, route_long_name, Service, direction_id, arrival_time, stop_sequence) %>% arrange(., route_short_name, Service, direction_id, arrival_time) %>% mutate(., Freq = difftime(arrival_time,lag(arrival_time),units='mins')) %>% na.omit() %>% filter(Freq>0)  %>% ungroup() %>% select(route_short_name, route_long_name, Service, Freq) %>% group_by(Service)

freq.mean <- bus.freq %>% summarise(Freq=mean(Freq))
most.freq <- bus.freq %>% filter(Freq==min(Freq)) %>% distinct() %>% mutate(Classification='Most Fequent') %>% ungroup()
leastfreq <- bus.freq %>% filter(Freq==max(Freq)) %>% distinct() %>% mutate(Classification='Least Fequent') %>% ungroup()

extreme.freq <- rbind(most.freq, leastfreq) %>% select(Service, Classification, Freq, Route=route_short_name, Route.name=route_long_name) %>% arrange(., Service, Freq) 
```

The mean frequencies across all routes for each type of service are as follows: `r slice(freq.mean,1)$Service`: `r slice(freq.mean,1)$Freq` mins, `r slice(freq.mean,2)$Service`: `r slice(freq.mean,2)$Freq` mins and `r slice(freq.mean,3)$Service`: `r slice(freq.mean,3)$Freq` mins. The routes with the highest and lowest frequencies routes for each type of service, along with their frequency in minutes are shown in the table below.

```{r}
kable(extreme.freq) %>% kable_styling(bootstrap_options = "striped", full_width = T)
```

```{r}
#calculating the unit of distance in the dataset
dist.calc <- bus.join %>% filter(route_short_name == 16) %>% filter(trip_id == min(trip_id)) %>% filter(stop_name == 'Redmond\'s Hill') %>% select(shape_dist_traveled) - bus.join %>% filter(route_short_name == 16) %>% filter(trip_id == min(trip_id)) %>% filter(stop_name == 'George\'s St') %>% select(shape_dist_traveled)

#calculating the route lengths
route.length <- bus.join %>% group_by(route_short_name, route_long_name, trip_id) %>% summarise(Dist.all = max(shape_dist_traveled)) %>% ungroup() %>% group_by(route_short_name, route_long_name) %>% summarise(Dist.mean = mean(Dist.all)/1000) %>% ungroup()
```

The units of distance travelled are not specified in the dataset. The distance on the 16 route from the George's Street stop to the Redmond's Hill stop is given in the dataset is `r dist.calc`. The distance between these two stops according to Google maps is 500m, so it appears that the unit of distance is meters. The mean and median route length are very similar at `r mean(route.length$Dist.mean)` km and `r median(route.length$Dist.mean)` km respectively. The shortest route is Route `r filter(route.length, Dist.mean==min(Dist.mean))$route_short_name`, `r filter(route.length, Dist.mean==min(Dist.mean))$route_long_name`, at `r filter(route.length, Dist.mean==min(Dist.mean))$Dist.mean` km. The longest route is Route `r filter(route.length, Dist.mean==max(Dist.mean))$route_short_name`, `r filter(route.length, Dist.mean==max(Dist.mean))$route_long_name`, at `r filter(route.length, Dist.mean==max(Dist.mean))$Dist.mean` km. The entire route map is shown below in Figure 6, overlaid over a map of Dublin [3]. The longest (green) and shortest (red) routes are also shown.
```{r fig.height=14, fig.width=14}
lat.long.all <- select(stops.df, Latitude=stop_lat, Longitude=stop_lon) %>% mutate(., Route='All Routes') %>% select(Route, Longitude, Latitude) %>% distinct()

lat.long.longest <- filter(bus.join, route_short_name==filter(route.length, Dist.mean==max(Dist.mean))$route_short_name) %>% select(., Route=route_short_name, Longitude=stop_lon, Latitude=stop_lat) %>% distinct()

lat.long.shortest <- filter(bus.join, route_short_name==filter(route.length, Dist.mean==min(Dist.mean))$route_short_name) %>% select(., Route=route_short_name, Longitude=stop_lon, Latitude=stop_lat) %>% distinct()

lat.long.combo <- rbind(lat.long.all, lat.long.longest, lat.long.shortest) 

# store bounding box coordinates - add a small amount so box extends slightly beyond the furthermost stop
dublin_bb <- c(left = (min(stops.df$stop_lon) - 0.01),
            bottom = (min(stops.df$stop_lat) - 0.01),
            right = (max(stops.df$stop_lon) + 0.01),
            top = (max(stops.df$stop_lat) + 0.01))

dublin_stamen <- get_stamenmap(bbox = dublin_bb, zoom = 11,  maptype = 'toner-lite')

ggmap(dublin_stamen, legend = 'right') + geom_point(data = lat.long.combo, mapping = aes(x = Longitude, y = Latitude, na.rm=TRUE, colour = Route, size=Route, alpha=Route)) +ggtitle('Figure 6: Map of All Dublin Bus Routes') + labs(x='Longitude', y='Latitude') + scale_colour_manual(values = c("red", "springgreen3", "deepskyblue3")) + scale_size_manual(values = c(2, 1.8, 0.7)) + scale_alpha_manual(values = c(1, 1, 0.5))
```

### 3.2 Pick a bus route
```{r}
home.route <- 49
bus.home <- bus.join %>% filter(route_short_name==home.route, Service=='Weekday')
```

I live in Harold's Cross and work in College Green so am picking Route `r home.route`, `r bus.home %>% select(route_long_name) %>% distinct()`. According to the data, direction_id = 1 is in the direction of `r bus.home %>% filter(direction_id==1) %>% select(stop_headsign) %>% distinct() `, so that's the direction I would get to work. Direction_id = 0 is in the direction of `r bus.home %>% filter(direction_id==0) %>% select(stop_headsign) %>% distinct()`, so that would be the direction I would get home from work. The stops in question are listed below.
```{r}
# Picking out stops and directions for work commute
home.stops <- bus.home %>%  filter(stop_id=='8220DB001296' | stop_id == '8220DB004521' | stop_id=='8220DB001339' | stop_id=='8220DB005192')
kable(home.stops %>% select(route_short_name, route_long_name, direction_id, stop_headsign, stop_id, stop_name) %>% distinct() ) %>% kable_styling(bootstrap_options = "striped", full_width = T)
```

```{r}
commute.trips <- home.stops %>% filter((direction_id==1 & stop_id=='8220DB005192' & arrival_time>as.hms('08:15:00') & arrival_time<as.hms('09:00:00')) | (direction_id==0 & stop_id=='8220DB004521' & arrival_time>as.hms('17:00:00') & arrival_time<as.hms('17:45:00'))) %>% select(trip_id)

length.route <- home.stops %>% inner_join(commute.trips, by='trip_id') %>% arrange(trip_id, stop_sequence) %>% group_by(direction_id, trip_id) %>% mutate(length.time=(lead(arrival_time) - arrival_time)/60) %>% mutate(length.dist=(lead(shape_dist_traveled) - shape_dist_traveled)/1000) %>% ungroup() %>% select(direction_id, stop_headsign, length.time, length.dist) %>% filter(!is.na(length.dist)) %>% distinct()
```

The trip into work should take `r filter(length.route, direction_id==1)$length.time` mins and `r filter(length.route, direction_id==1)$length.dist` km. The trip home from work should take `r filter(length.route, direction_id==0)$length.time` mins and `r filter(length.route, direction_id==0)$length.dist` km. Neither of these times appear to take into consideration traffic so I would have to look at timings in practice.

If I needed to arrive for work at between 8.15 and 9am, and want to get a bus home from work between 5pm and 5.45 pm, I'll have to get one of the services below. As you can see, tehre are several options for coming home from work, but only two going to work in the morning.

```{r}
kable(inner_join(home.stops, commute.trips, by='trip_id') %>% mutate(Journey=case_when(stop_headsign=='Pearse St' ~ 'Commute to work', stop_headsign=='The Square' ~ 'Commute to home') ) %>% select(Service, Journey, Direction=stop_headsign, stop_name,  arrival_time)) %>% kable_styling(bootstrap_options = "striped", full_width = T)
```

The times for the earliest and latest buses on Route `r home.route` are below.
```{r}
kable(bus.join %>% group_by(Service, stop_headsign) %>% filter(!is.na(arrival_time), route_short_name==home.route) %>% summarise(earliest.bus=min(arrival_time), latest.bus= max(arrival_time)))  %>% kable_styling(bootstrap_options = "striped", full_width = T)
```

I like exploring Dublin on a Saturday but I don't have a car and I don't like walking between stops to change bus. Here I'll look at everywhere that I can get to, using buses that interconnect with my chosen route `r home.route`. This analysis can easily be changed to look at other routes by changing the bus route chosen above.
```{r}
# First I select all the stops on my route, than all the other buses that also stop at those stops
bus.join.sat <- filter(bus.join, Service=='Saturday')
stops.home <- bus.join.sat %>% filter(route_short_name==home.route)

stops.intersect <- inner_join(stops.home, bus.join.sat, by='stop_id', suffix=c('.home', '.intersect')) %>% select(route_short_name.intersect) %>% distinct() %>% inner_join(bus.join.sat, by = c("route_short_name.intersect" = "route_short_name")) %>% select(Longitude=stop_lon, Latitude=stop_lat) %>% distinct()

intersect.routes <- rbind((stops.intersect %>% mutate(Route=paste('All Routes Intersecting with Route', home.route))), (stops.home %>% select(Longitude=stop_lon, Latitude=stop_lat) %>% mutate(Route=paste('Route', home.route)))) %>% select(Route, Longitude, Latitude)
```

```{r fig.height=14, fig.width=14}
# store bounding box coordinates - add a small amount so box extends slightly beyond the furthermost stop
dublin_intersect_bb <- c(left = (min(stops.intersect$Longitude) - 0.01),
            bottom = (min(stops.intersect$Latitude) - 0.01),
            right = (max(stops.intersect$Longitude) + 0.01),
            top = (max(stops.intersect$Latitude) + 0.01))

dublin_intersect_stamen <- get_stamenmap(bbox = dublin_intersect_bb, zoom = 11,  maptype = 'toner-lite')

ggmap(dublin_intersect_stamen, legend = 'right') + geom_point(data = intersect.routes, mapping = aes(x = Longitude, y = Latitude, na.rm=TRUE, colour = Route, size=Route, alpha=Route)) +ggtitle(paste('Figure 7: Map of All Dublin Bus Routes intersecting with Route ',home.route)) + labs(x='Longitude', y='Latitude') + scale_colour_manual(values = c("deepskyblue3", "purple")) + scale_size_manual(values = c(1, 1)) + scale_alpha_manual(values = c(0.5, 1))
```



## Task 4: Footfall
### 4.1: Footfall cameras
```{r include=FALSE}
#loading in the footfall data
url.footfall <- 'https://data.smartdublin.ie/dataset/8204be0a-6348-459e-96e9-65bb75600ec3/resource/384fe47a-2f25-4f52-8fc5-8e61899951e9/download/pedestrianfootfall2013.ods'
download.file(url.footfall, 'footfall_ods.ods')
sheet.names <- ods_sheets('pedestrianfootfall2013.ods')
```

```{r}
#getting list of camera locations
camera.locations <- read_ods('pedestrianfootfall2013.ods',sheet.names[1])[1]
colnames(camera.locations) <- c('Loc') 
#camera.locations %>% filter(., grepl("Entrance Name:", Loc, fixed = TRUE)) 
```

```{r}
# reading in data from the selected cameras 
camera1.name <- 'Entrance Name: Capel St at Mullen'
camera2.name <- 'Entrance Name: Grafton St at M&S'

camera1.all <- data.frame(matrix(ncol = 15, nrow = 0))
colnames(camera1.all) <- c('Time', 'Mon.In', 'Mon.Out', 'Tue.In', 'Tue.Out', 'Wed.In', 'Wed.Out', 'Thurs.In', 'Thurs.Out', 'Fri.In', 'Fri.Out', 'Sat.In', 'Sat.Out', 'Sun.In', 'Sun.Out')

camera2.all <- data.frame(matrix(ncol = 15, nrow = 0))
colnames(camera2.all) <- c('Time', 'Mon.In', 'Mon.Out', 'Tue.In', 'Tue.Out', 'Wed.In', 'Wed.Out', 'Thurs.In', 'Thurs.Out', 'Fri.In', 'Fri.Out', 'Sat.In', 'Sat.Out', 'Sun.In', 'Sun.Out')

for(week.i in 1:length(sheet.names))
{
  f1 <- read_ods('pedestrianfootfall2013.ods',sheet.names[week.i])
  
  if (ncol(f1)==15)
  {
    colnames(f1) <- c('Time', 'Mon.In', 'Mon.Out', 'Tue.In', 'Tue.Out', 'Wed.In', 'Wed.Out', 'Thurs.In', 'Thurs.Out', 'Fri.In', 'Fri.Out', 'Sat.In', 'Sat.Out', 'Sun.In', 'Sun.Out')
  } else {
    colnames(f1) <- c('Time', 'Mon.In', 'Mon.Out', 'Tue.In', 'Tue.Out', 'Wed.In', 'Wed.Out', 'Thurs.In', 'Thurs.Out', 'Fri.In', 'Fri.Out', 'Sat.In', 'Sat.Out', 'Sun.In', 'Sun.Out', 'unknown1', 'unknown2', 'unknown3')
  }
  f1 <- select(f1, Time, Mon.In, Mon.Out, Tue.In, Tue.Out, Wed.In, Wed.Out, Thurs.In, Thurs.Out, Fri.In, Fri.Out, Sat.In, Sat.Out, Sun.In, Sun.Out)
  
  camera1.row <- which(grepl(camera1.name, f1$Time))
  camera2.row <- which(grepl(camera2.name, f1$Time))

  camera1.slice <- slice(f1, (camera1.row+3):(camera1.row+26)) %>% mutate(Week=sheet.names[week.i])
  camera2.slice <- slice(f1, (camera2.row+3):(camera2.row+26)) %>% mutate(Week=sheet.names[week.i])
  
  camera1.all <- rbind(camera1.all, camera1.slice)
  camera2.all <- rbind(camera2.all, camera2.slice)
  
  f1 <- NULL
}
```

```{r}
# Cleaning up fields, some character fields should be numeric, standardising field names etc
camera1.all$Mon.In <- as.numeric(camera1.all$Mon.In)
camera1.all$Mon.Out <- as.numeric(camera1.all$Mon.Out)
camera1.all$Tue.In <- as.numeric(camera1.all$Tue.In)
camera1.all$Tue.Out <- as.numeric(camera1.all$Tue.Out)
camera1.all$Wed.In <- as.numeric(camera1.all$Wed.In)
camera1.all$Wed.Out <- as.numeric(camera1.all$Wed.Out)
camera1.all$Thurs.In <- as.numeric(camera1.all$Thurs.In)
camera1.all$Thurs.Out <- as.numeric(camera1.all$Thurs.Out)
camera1.all$Fri.In <- as.numeric(camera1.all$Fri.In)
camera1.all$Fri.Out <- as.numeric(camera1.all$Fri.Out)
camera1.all$Sat.In <- as.numeric(camera1.all$Sat.In)
camera1.all$Sat.Out <- as.numeric(camera1.all$Sat.Out)
camera1.all$Sun.In <- as.numeric(camera1.all$Sun.In)
camera1.all$Sun.Out <- as.numeric(camera1.all$Sun.Out)

camera2.all$Mon.In <- as.numeric(camera2.all$Mon.In)
camera2.all$Mon.Out <- as.numeric(camera2.all$Mon.Out)
camera2.all$Tue.In <- as.numeric(camera2.all$Tue.In)
camera2.all$Tue.Out <- as.numeric(camera2.all$Tue.Out)
camera2.all$Wed.In <- as.numeric(camera2.all$Wed.In)
camera2.all$Wed.Out <- as.numeric(camera2.all$Wed.Out)
camera2.all$Thurs.In <- as.numeric(camera2.all$Thurs.In)
camera2.all$Thurs.Out <- as.numeric(camera2.all$Thurs.Out)
camera2.all$Fri.In <- as.numeric(camera2.all$Fri.In)
camera2.all$Fri.Out <- as.numeric(camera2.all$Fri.Out)
camera2.all$Sat.In <- as.numeric(camera2.all$Sat.In)
camera2.all$Sat.Out <- as.numeric(camera2.all$Sat.Out)
camera2.all$Sun.In <- as.numeric(camera2.all$Sun.In)
camera2.all$Sun.Out <- as.numeric(camera2.all$Sun.Out)

# there was inconsistency in sheet naming so fixing this
camera1.all$Week <- gsub('Sheet13', 'Week_13', camera1.all$Week)
camera2.all$Week <- gsub('Sheet13', 'Week_13', camera2.all$Week)

camera1.all$Week <- gsub('Week_1\\>', 'Week_01', camera1.all$Week)
camera1.all$Week <- gsub('Week_2\\>', 'Week_02', camera1.all$Week)
camera1.all$Week <- gsub('Week_3\\>', 'Week_03', camera1.all$Week)
camera1.all$Week <- gsub('Week_4\\>', 'Week_04', camera1.all$Week)
camera1.all$Week <- gsub('Week_5\\>', 'Week_05', camera1.all$Week)
camera1.all$Week <- gsub('Week_6\\>', 'Week_06', camera1.all$Week)
camera1.all$Week <- gsub('Week_7\\>', 'Week_07', camera1.all$Week)
camera1.all$Week <- gsub('Week_8\\>', 'Week_08', camera1.all$Week)
camera1.all$Week <- gsub('Week_9\\>', 'Week_09', camera1.all$Week)
camera2.all$Week <- gsub('Week_1\\>', 'Week_01', camera2.all$Week)
camera2.all$Week <- gsub('Week_2\\>', 'Week_02', camera2.all$Week)
camera2.all$Week <- gsub('Week_3\\>', 'Week_03', camera2.all$Week)
camera2.all$Week <- gsub('Week_4\\>', 'Week_04', camera2.all$Week)
camera2.all$Week <- gsub('Week_5\\>', 'Week_05', camera2.all$Week)
camera2.all$Week <- gsub('Week_6\\>', 'Week_06', camera2.all$Week)
camera2.all$Week <- gsub('Week_7\\>', 'Week_07', camera2.all$Week)
camera2.all$Week <- gsub('Week_8\\>', 'Week_08', camera2.all$Week)
camera2.all$Week <- gsub('Week_9\\>', 'Week_09', camera2.all$Week)
```

```{r}
# Summing in and out
camera1.all <- camera1.all %>% mutate(Mon=Mon.In+Mon.Out) %>% mutate(Tue=Tue.In+Tue.Out) %>% mutate(Wed=Wed.In+Wed.Out) %>% mutate(Thurs=Thurs.In+Thurs.Out) %>% mutate(Fri=Fri.In+Fri.Out) %>% mutate(Sat=Sat.In+Sat.Out) %>% mutate(Sun=Sun.In+Sun.Out) %>% mutate(All=Mon+Tue+Wed+Thurs+Fri+Sat+Sun) %>% mutate(Week.fact=as.factor(Week))

camera2.all <- camera2.all %>% mutate(Mon=Mon.In+Mon.Out) %>% mutate(Tue=Tue.In+Tue.Out) %>% mutate(Wed=Wed.In+Wed.Out) %>% mutate(Thurs=Thurs.In+Thurs.Out) %>% mutate(Fri=Fri.In+Fri.Out) %>% mutate(Sat=Sat.In+Sat.Out) %>% mutate(Sun=Sun.In+Sun.Out) %>% mutate(All=Mon+Tue+Wed+Thurs+Fri+Sat+Sun) %>% mutate(Week.fact=as.factor(Week))
```
```{r}
camera1.sum.week <- camera1.all %>% group_by(Week.fact) %>% summarise(Footfall=sum(All)) %>% mutate(Camera = gsub("Entrance Name: ","", camera1.name))
camera2.sum.week <- camera2.all %>% group_by(Week.fact) %>% summarise(Footfall=sum(All)) %>% mutate(Camera = gsub("Entrance Name: ","", camera2.name))
```

The footfall in Capel Street (average per week over 2013 is `r mean(camera1.sum.week$Footfall)`) tends to be much lower than that in Grafton street (average per week over 2013 is `r mean(camera2.sum.week$Footfall)`), which would be expected as Grafton Street is one of the busiest shopping streets in teh country. We can see the volume of footfall over the course of the year 2013 in Figure 8 below.

```{r fig.height=14, fig.width=14}
ggplot(data=rbind(camera1.sum.week, camera2.sum.week), aes(x=Week.fact, y=Footfall, group=Camera, colour = Camera)) + geom_point() + geom_line() + facet_grid(rows=vars(Camera), scales='free_y') + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + labs(x = 'Week of Year 2013', y = 'Footfall', title='Figure 8: Footfall for weeks of year 2013') + scale_y_continuous(labels = comma)
```

Both plots follow similar patterns of being lower at the start of the year and higher at the end of the year.

```{r}
# Working out minumum and maximum footfall times
c1max <- camera1.all %>% summarise(max(Mon), max(Tue), max(Wed), max(Thurs), max(Fri), max(Sat), max(Sun)) %>% max()
c1max.result <- camera1.all %>% filter(Mon==c1max | Tue==c1max | Wed==c1max | Thurs==c1max | Fri==c1max | Sat==c1max | Sun==c1max) %>% select(Week, Time, Mon, Tue, Wed, Thurs, Fri, Sat, Sun)

if(camera1.all %>% summarise(max(Mon))==c1max)
{
  c1max.day <- 'Monday'
} else if (camera1.all %>% summarise(max(Tue))==c1max){
  c1max.day <- 'Tuesday'
} else if (camera1.all %>% summarise(max(Wed))==c1max){
  c1max.day <- 'Wednesday'
} else if (camera1.all %>% summarise(max(Thurs))==c1max){
  c1max.day <- 'Thursday'
} else if (camera1.all %>% summarise(max(Fri))==c1max){
  c1max.day <- 'Friday'
} else if (camera1.all %>% summarise(max(Sat))==c1max){
  c1max.day <- 'Saturday'
} else if (camera1.all %>% summarise(max(Sun))==c1max){
  c1max.day <- 'Sunday'
} 

c2max <- camera2.all %>% summarise(max(Mon), max(Tue), max(Wed), max(Thurs), max(Fri), max(Sat), max(Sun)) %>% max()
c2max.result <- camera2.all %>% filter(Mon==c2max | Tue==c2max | Wed==c2max | Thurs==c2max | Fri==c2max | Sat==c2max | Sun==c2max) %>% select(Week, Time, Mon, Tue, Wed, Thurs, Fri, Sat, Sun)

if(camera2.all %>% summarise(max(Mon))==c2max)
{
  c2max.day <- 'Monday'
} else if (camera2.all %>% summarise(max(Tue))==c2max){
  c2max.day <- 'Tuesday'
} else if (camera2.all %>% summarise(max(Wed))==c2max){
  c2max.day <- 'Wednesday'
} else if (camera2.all %>% summarise(max(Thurs))==c2max){
  c2max.day <- 'Thursday'
} else if (camera2.all %>% summarise(max(Fri))==c2max){
  c2max.day <- 'Friday'
} else if (camera2.all %>% summarise(max(Sat))==c2max){
  c2max.day <- 'Saturday'
} else if (camera2.all %>% summarise(max(Sun))==c2max){
  c2max.day <- 'Sunday'
} 

c1min <- camera1.all %>% filter(Week!='Week_01') %>% summarise(min(Mon), min(Tue), min(Wed), min(Thurs), min(Fri), min(Sat), min(Sun)) %>% min()
c1min.result <- camera1.all %>% filter(Week!='Week_01') %>% filter(Mon==c1min | Tue==c1min | Wed==c1min | Thurs==c1min | Fri==c1min | Sat==c1min | Sun==c1min) %>% select(Week, Time, Mon, Tue, Wed, Thurs, Fri, Sat, Sun)

# Checking if camera working
# camera1.all %>% filter(Week=='Week_25')

c2min <- camera2.all %>% filter(Week!='Week_01') %>% summarise(min(Mon), min(Tue), min(Wed), min(Thurs), min(Fri), min(Sat), min(Sun)) %>% min()
c2min.result <- camera2.all %>% filter(Week!='Week_01') %>% filter(Mon==c2min | Tue==c2min | Wed==c2min | Thurs==c2min | Fri==c2min | Sat==c2min | Sun==c2min) %>% select(Week, Time, Mon, Tue, Wed, Thurs, Fri, Sat, Sun)

if(camera2.all %>% summarise(min(Mon))==c2min)
{
  c2min.day <- 'Monday'
} else if (camera2.all %>% summarise(min(Tue))==c2min){
  c2min.day <- 'Tuesday'
} else if (camera2.all %>% summarise(min(Wed))==c2min){
  c2min.day <- 'Wednesday'
} else if (camera2.all %>% summarise(min(Thurs))==c2min){
  c2min.day <- 'Thursday'
} else if (camera2.all %>% summarise(min(Fri))==c2min){
  c2min.day <- 'Friday'
} else if (camera2.all %>% summarise(min(Sat))==c2min){
  c2min.day <- 'Saturday'
} else if (camera2.all %>% summarise(min(Sun))==c2min){
  c2min.day <- 'Sunday'
} 

# Checking if camera working
# camera2.all %>% filter(Week==c2min.result$Week)
```

The single busiest time for the `r gsub("Entrance Name: ","", camera1.name)` camera is the `r c1max.day` in `r c1max.result$Week` at `r c1max.result$Time` with `r c1max` people passing by in either direction.

The single busiest time for the `r gsub("Entrance Name: ","", camera2.name)` camera is the `r c2max.day` in `r c2max.result$Week` at `r c2max.result$Time` with `r c2max` people passing by in either direction. For the `r gsub("Entrance Name: ","", camera2.name)` camera, there is only one time period in 2013 where nobody passes the camera location in the hour measured, `r c2min.day` in `r c2min.result$Week` at `r c2min.result$Time`. This seems unusual as this is a Saturday night so you would expect there to be foot traffic on Grafton Street. The times either side of this have footfall so it doesn't appear that that camera was down. There are several times on `r gsub("Entrance Name: ","", camera1.name)` camera that there is zero footfall. Most are between 23:00 and 4:00, but there is again an unlikely time. It could be that, occasionally, busy street are empty of foot traffic. Week 1 is excluded from the minimum analysis, as there is not a Monday in Week 1.


### Task 2.2 Ambient Sound
I chose an ambient sound dataset that is gathered by Dublin City Council. The sound levels are measured in Leq, which is the average sound level over the period of measurement [4].  It is captured in 5 minute increments that I have rolled up into average for a one hour period in order to be able to compare with the footfall data in questions 4.1. I am picking the camera location of `r gsub("Entrance Name: ","", camera1.name)` and the ambient sound location of Chancery Park as they are very close to each other. I will investigate whether the level of footfall is related to the levels of ambient sound.

```{r include=FALSE}
ambient.sound.url <- 'https://data.smartdublin.ie/dataset/a52fbbe2-1bff-4897-84af-34945f6fc8de/resource/30cb9275-f9fb-432a-af48-0a5f7ab462a4/download/chancerypark2013.zip'
download.file(ambient.sound.url, destfile = 'ambient_sound.zip')
unzip('ambient_sound.zip', exdir = './ambient_sound')
```

```{r}
# I need to loop through all dates in the year to pull out each text file. Here I create a df of all required components
months <- format(ISOdate(2013,1:12,1),"%B")

dates.df <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(dates.df) <- c('year', 'month.char', 'month.num', 'day')

for (i in 1:12)
{
  date.i <- data.frame(matrix(ncol = 4, nrow = 1))
  colnames(date.i) <- c('year', 'month.char', 'month.num', 'day')
  
  date.i$year <- '2013'
  date.i$month.char <- months[i]
  if(nchar(as.character(i))==1)
  {
    date.i$month.num <- paste('0', i, sep='')
  } else {
    date.i$month.num <- i
  }
  if(months[i]=='December')
  {
   date.i$day = 28 #the last 3 days of the year are excluded from the footfall dataset
  } else {
  date.i$day <- as.Date(paste('2013','-', i ,'-01', sep=''), '%Y-%m-%d') %>% days_in_month()
  }
  dates.df <- rbind(dates.df, date.i)
  
  date.i <- NULL
}

```

```{r}
# running through each file and loading
sound.all <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(sound.all) <- c('week', 'day.week', 'Time', 'mean.noise.level')

for (i.month in 1:nrow(dates.df))
{
  for (i.day in 1:dates.df$day[i.month])
  {
    
    if(nchar(as.character(i.day))==1)
     {
      sound.file <- paste('ambient_sound/',dates.df$year[i.month], '/', dates.df$month.char[i.month], '/DCC-NOISE-001011-2013-', dates.df$month.num[i.month], '-0', i.day, 'T00-00-00.txt', sep='')
    } else {
     sound.file <- paste('ambient_sound/2013/', dates.df$month.char[i.month], '/DCC-NOISE-001011-2013-', dates.df$month.num[i.month], '-', i.day, 'T00-00-00.txt', sep='')
    }
    if (!file.exists(sound.file)) next

    sound.read <- read.table(sound.file) 
    sound.df <- sound.read %>% select(date.sound=V1, time=V2, noise.level=V3) 

    sound.all <- rbind.data.frame(sound.all, sound.df)
    
    sound.df<- NULL
  }
}
```


```{r}
# Fixing punctuation and tidying up
sound.tidy <- sound.all
sound.tidy$time <-  gsub(",","", sound.tidy$time)
sound.tidy$noise.level <-  as.numeric(gsub(",","", sound.tidy$noise.level))
sound.tidy$date.sound <-  as.Date(sound.tidy$date.sound, format='%d/%m/%Y')

sound.tidy$Time <- paste(format(strptime(sound.tidy$time,format='%H:%M:%S'), "%H"), ':00', sep='')
sound.tidy.summary <- sound.tidy %>% group_by(date.sound, Time) %>% summarise(mean.noise.level=mean(noise.level)) 

sound.tidy.summary$Week <- paste('Week', strftime(sound.tidy.summary$date.sound, "%V"), sep='_')
sound.tidy.summary$day.week <- format(as.Date(sound.tidy.summary$date.sound), "%A") 

sound.tidy.all <- sound.tidy.summary[2:5] 
```


```{r}
# The data is arranged longways. Changing to day per column in order to compare to footfall data
sound.split <- split(sound.tidy.all, sound.tidy.all$day.week)

sound.mon <- sound.split$Monday %>% select(., Week, Time, mon.noise=mean.noise.level)
sound.tue <- sound.split$Tuesday %>% select(Week, Time, tue.noise=mean.noise.level)
sound.wed <- sound.split$Wednesday %>% select(Week, Time, wed.noise=mean.noise.level)
sound.thurs <- sound.split$Thursday %>% select(Week, Time, thurs.noise=mean.noise.level)
sound.fri <- sound.split$Friday %>% select(Week, Time, fri.noise=mean.noise.level)
sound.sat <- sound.split$Saturday %>% select(Week, Time, sat.noise=mean.noise.level)
sound.sun <- sound.split$Sunday %>% select(Week, Time, sun.noise=mean.noise.level)

sound.new <- sound.mon %>% full_join(sound.tue, by=c('Week', 'Time')) %>% full_join(sound.wed, by=c('Week', 'Time')) %>% full_join(sound.thurs, by=c('Week', 'Time')) %>% full_join(sound.fri, by=c('Week', 'Time')) %>% full_join(sound.sat, by=c('Week', 'Time')) %>% full_join(sound.sun, by=c('Week', 'Time')) %>% arrange(Week, Time)  %>% mutate(Week.fact=as.factor(Week))

sound.new[is.na(sound.new)] <- 0

sound.all <- sound.new %>% mutate(All=mon.noise+tue.noise+wed.noise+thurs.noise+fri.noise+sat.noise+sun.noise)
```

Comparing footfall and ambient noise levels for each week of the year in Figure 9 below doesn't give us that must insight. The footfall traffic increases over the year, but the ambient noise levels are fairly steady on a macro scale (there are some outliers where data is missing).

```{r fig.height=8, fig.width=14}
p1 <- ggplot(data=camera1.sum.week, aes(x=Week.fact, y=Footfall, group=1))  + geom_point() + geom_line() + labs(x = '', y = paste('Footfall Levels at', gsub("Entrance Name: ","", camera1.name)), title='Figure 9: Ambient Noise and Footfall Levels over Year 2013')  + theme(axis.text.x = element_text(angle = 60, hjust = 1))
p2 <- ggplot(data=sound.week, aes(x=Week.fact, y=Sound_Level, group=1)) + geom_point() + geom_line() + labs(x = 'Weeks of Year', y = 'Ambient Noise Levels at Chancery Park')  + theme(axis.text.x = element_text(angle = 60, hjust = 1))
grid.arrange(p1, p2, nrow = 2) 
```

Choosing a week at random, week 12, Figure 10 below compares the footfall and ambient sound across the week. We can see that they follow a similar pattern, both are low in the early hours of the morning and late at night and peak during the day. Both the footfall and ambient noise have higher levels are higher over Friday and Saturday nights than similar times other days of the week. This is to be expected because there are many bars in this area.

```{r fig.height=14, fig.width=14}
camera1.all.w12.mon  <- camera1.all %>% filter(Week=='Week_12') %>% select(Time, Mon) %>% mutate(Label='Footfall')
sound.all.w12.mon  <- sound.all %>% filter(Week=='Week_12') %>% select(Time, Mon=mon.noise) %>% mutate(Label='Ambient Noise')

camera1.all.w12.tue  <- camera1.all %>% filter(Week=='Week_12') %>% select(Time, Tue) %>% mutate(Label='Footfall')
sound.all.w12.tue  <- sound.all %>% filter(Week=='Week_12') %>% select(Time, Tue=tue.noise) %>% mutate(Label='Ambient Noise')

camera1.all.w12.wed  <- camera1.all %>% filter(Week=='Week_12') %>% select(Time, Wed) %>% mutate(Label='Footfall')
sound.all.w12.wed  <- sound.all %>% filter(Week=='Week_12') %>% select(Time, Wed=wed.noise) %>% mutate(Label='Ambient Noise')

camera1.all.w12.thurs  <- camera1.all %>% filter(Week=='Week_12') %>% select(Time, Thurs) %>% mutate(Label='Footfall')
sound.all.w12.thurs  <- sound.all %>% filter(Week=='Week_12') %>% select(Time, Thurs=thurs.noise) %>% mutate(Label='Ambient Noise')

camera1.all.w12.fri  <- camera1.all %>% filter(Week=='Week_12') %>% select(Time, Fri) %>% mutate(Label='Footfall')
sound.all.w12.fri  <- sound.all %>% filter(Week=='Week_12') %>% select(Time, Fri=fri.noise) %>% mutate(Label='Ambient Noise')

camera1.all.w12.sat  <- camera1.all %>% filter(Week=='Week_12') %>% select(Time, Sat) %>% mutate(Label='Footfall')
sound.all.w12.sat  <- sound.all %>% filter(Week=='Week_12') %>% select(Time, Sat=sat.noise) %>% mutate(Label='Ambient Noise')

camera1.all.w12.sun  <- camera1.all %>% filter(Week=='Week_12') %>% select(Time, Sun) %>% mutate(Label='Footfall')
sound.all.w12.sun  <- sound.all %>% filter(Week=='Week_12') %>% select(Time, Sun=sun.noise) %>% mutate(Label='Ambient Noise')

#theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank())

p1 <- ggplot(data=rbind(camera1.all.w12.mon, sound.all.w12.mon), aes(x=Time, y=Mon, group=Label, colour = Label)) + geom_point() + geom_line() + facet_grid(rows=vars(Label), scales='free_y') + labs(x = 'Monday', y = '') + theme(legend.position="none", axis.text.x=element_blank(), strip.text.x = element_blank(), strip.text.y = element_blank(), axis.ticks.x=element_blank(), axis.ticks.y=element_blank())

p2 <- ggplot(data=rbind(camera1.all.w12.tue, sound.all.w12.tue), aes(x=Time, y=Tue, group=Label, colour = Label)) + geom_point() + geom_line() + facet_grid(rows=vars(Label), scales='free_y') + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = 'Tuesday', y = '') + theme(legend.position="none", axis.text.x=element_blank(), strip.text.x = element_blank(), strip.text.y = element_blank(), axis.text.y=element_blank(), axis.ticks.x=element_blank(), axis.ticks.y=element_blank())

p3 <- ggplot(data=rbind(camera1.all.w12.wed, sound.all.w12.wed), aes(x=Time, y=Wed, group=Label, colour = Label)) + geom_point() + geom_line() + facet_grid(rows=vars(Label), scales='free_y') + labs(x = 'Wednesday', y = '')+ theme(legend.position="none", axis.text.x=element_blank(), strip.text.x = element_blank(), strip.text.y = element_blank(), axis.text.y=element_blank(), axis.ticks.x=element_blank(), axis.ticks.y=element_blank())

p4 <- ggplot(data=rbind(camera1.all.w12.thurs, sound.all.w12.thurs), aes(x=Time, y=Thurs, group=Label, colour = Label)) + geom_point() + geom_line() + facet_grid(rows=vars(Label), scales='free_y') + labs(x = 'Thursday', y = '')+ theme(legend.position="none", axis.text.x=element_blank(), strip.text.x = element_blank(), strip.text.y = element_blank(), axis.text.y=element_blank(), axis.ticks.x=element_blank(), axis.ticks.y=element_blank())

p5 <- ggplot(data=rbind(camera1.all.w12.fri, sound.all.w12.fri), aes(x=Time, y=Fri, group=Label, colour = Label)) + geom_point() + geom_line() + facet_grid(rows=vars(Label), scales='free_y')  + labs(x = 'Friday', y = '')+ theme(legend.position="none", axis.text.x=element_blank(), strip.text.x = element_blank(), strip.text.y = element_blank(), axis.text.y=element_blank(), axis.ticks.x=element_blank(), axis.ticks.y=element_blank())

p6 <- ggplot(data=rbind(camera1.all.w12.sat, sound.all.w12.sat), aes(x=Time, y=Sat, group=Label, colour = Label)) + geom_point() + geom_line() + facet_grid(rows=vars(Label), scales='free_y')  + labs(x = 'Saturday', y = '')+ theme(legend.position="none", axis.text.x=element_blank(), strip.text.x = element_blank(), strip.text.y = element_blank(), axis.text.y=element_blank(), axis.ticks.x=element_blank(), axis.ticks.y=element_blank())

p7 <- ggplot(data=rbind(camera1.all.w12.sun, sound.all.w12.sun), aes(x=Time, y=Sun, group=Label, colour = Label)) + geom_point() + geom_line() + facet_grid(rows=vars(Label), scales='free_y')  + labs(x = 'Sunday', y = '')+ theme(legend.position="none", axis.text.x=element_blank(), strip.text.x = element_blank(), axis.text.y=element_blank(), axis.ticks.x=element_blank(), axis.ticks.y=element_blank())

grid.arrange(p1, p2, p3, p4, p5, p6, p7, nrow = 1, top = textGrob("Figure 10: Footfall and Ambient Noise for Each Day of the Week in Week 12 2013",gp=gpar(fontsize=20,font=3))) 
```

Again, Figure 9 only looks at part of the data, the view is more detailed than Figure 8, but it's only for 1 week. In figure 10 below, we attempt to look at the whole year. Here, we normalise the footfall data by dividing all values by the highest footfall value for the whole year. We similarly normalise the ambient noise data by dividing all values by the highest noise level in the year. We then look at the ratio of normalised noise level to normalised footfall, and plot them in Figure 10 below. 

We can see that for most of the day, the ratio is fairly steady. This would imply that the footfall and ambient noise levels are varying evenly with each other. However, during the night, there is a wider variation in the ratio, and the value of the ratio increases. An increacing value of the ratio would mean that the ambient noise is increasing faster than the footfall. There are several thngs that could cause this, people leaving pubs might be disproportionally loud, so would increase the ambient sound more than the footfall. There could also be some building work or bin collection happening during the night time hours.

```{r}
# Looking at the ratio of footfall to ambient noise
# Normalising the footfall levels by dividing my max over the year
camera1.all.norm <- camera1.all %>% mutate(Mon.norm=Mon/c1max) %>% mutate(Tue.norm=Tue/c1max) %>% mutate(Wed.norm=Wed/c1max) %>% mutate(Thurs.norm=Thurs/c1max) %>% mutate(Fri.norm=Fri/c1max) %>% mutate(Sat.norm=Sat/c1max) %>% mutate(Sun.norm=Sun/c1max) %>% select(Week.fact, Time, Mon.norm, Tue.norm, Wed.norm, Thurs.norm, Fri.norm, Sat.norm, Sun.norm)

noise.max <- sound.all %>% summarise(max(mon.noise), max(tue.noise), max(wed.noise), max(thurs.noise), max(fri.noise), max(sat.noise), max(sun.noise)) %>% max()

noise.all.norm <- sound.all %>% mutate(Mon.norm=mon.noise/c1max) %>% mutate(Tue.norm=tue.noise/c1max) %>% mutate(Wed.norm=wed.noise/c1max) %>% mutate(Thurs.norm=thurs.noise/c1max) %>% mutate(Fri.norm=fri.noise/c1max) %>% mutate(Sat.norm=sat.noise/c1max) %>% mutate(Sun.norm=sun.noise/c1max) %>% select(Week.fact, Time, Mon.norm, Tue.norm, Wed.norm, Thurs.norm, Fri.norm, Sat.norm, Sun.norm)

norm.ratio <- left_join(noise.all.norm, camera1.all.norm, by=c('Week.fact', 'Time'), suffix=c('.noise','.footfall')) %>% mutate(Mon.norm=Mon.norm.noise/Mon.norm.footfall) %>% mutate(Tue.norm=Tue.norm.noise/Tue.norm.footfall) %>% mutate(Wed.norm=Wed.norm.noise/Wed.norm.footfall) %>% mutate(Thurs.norm=Thurs.norm.noise/Thurs.norm.footfall) %>% mutate(Fri.norm=Fri.norm.noise/Fri.norm.footfall) %>% mutate(Sat.norm=Sat.norm.noise/Sat.norm.footfall) %>% mutate(Sun.norm=Sun.norm.noise/Sun.norm.footfall) %>% mutate(Week.time=paste(Week.fact, Time)) %>% select(Week.fact, Time, Week.time, Mon.norm, Tue.norm, Wed.norm, Thurs.norm, Fri.norm, Sat.norm, Sun.norm)
norm.ratio
```

```{r  fig.height=14, fig.width=14}
p1 <- ggplot(data=norm.ratio, aes(x=Time, y=Mon.norm)) +  geom_point(shape=1) + labs(x = '', y = 'Monday', title='Figure 10: Ambient Noise to Footfall Ratio')
p2 <- ggplot(data=norm.ratio, aes(x=Time, y=Tue.norm)) +  geom_point(shape=1) + labs(x = '', y = 'Tuesday') 
p3 <- ggplot(data=norm.ratio, aes(x=Time, y=Wed.norm)) +  geom_point(shape=1) + labs(x = '', y = 'Wednesday') 
p4 <- ggplot(data=norm.ratio, aes(x=Time, y=Thurs.norm)) +  geom_point(shape=1) + labs(x = '', y = 'Thursday')  
p5 <- ggplot(data=norm.ratio, aes(x=Time, y=Fri.norm)) +  geom_point(shape=1) + labs(x = '', y = 'Friday')  
p6 <- ggplot(data=norm.ratio, aes(x=Time, y=Sat.norm)) +  geom_point(shape=1) + labs(x = '', y = 'Saturday')  
p7 <- ggplot(data=norm.ratio, aes(x=Time, y=Sun.norm)) +  geom_point(shape=1) + labs(x = 'Time', y = 'Sunday')  
grid.arrange(p1, p2, p3, p4, p5, p6, p7, nrow = 7) 
```


# 5 Reflection
## 5.1 Analytic
Here I analyse the the history of my log console for the duration of this assignment.
```{r}
#The history required spans multiple files so will add all here and remove duplicates and rows about other subjects
file.list <- list('history_database/history_database_2211','history_database/history_database2411','history_database/history_database2511', 'history_database/history_database_20181114_end',  'history_database/history_database_20181105_end', 'history_database/history_database_20181103','history_database/history_database_20181031')

history.all <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(sound.all) <- c('date.mili', 'code')

for (i.hist in 1:length(file.list))
{
  history.read <- read.delim(file.list[[i.hist]], header=FALSE, sep=':', stringsAsFactors=FALSE) 
  history.all <- rbind.data.frame(history.all, history.read)
  
  history.read<- NULL
}

history <- history.all %>% distinct() %>% mutate(hist.date.time.base=as.POSIXct(as.numeric(V1)/1000, origin=as.Date('1970-01-01', "%Y-%m-%d"), format = "%Y-%m-%d %H:%M:%OS")) %>% mutate(hist.date=format(hist.date.time.base, format = "%Y-%m-%d")) %>% mutate(hist.time=format(hist.date.time.base, format = "%H")) %>% mutate(hist.date.time=format(hist.date.time.base, format = "%Y-%m-%d: %H")) %>% mutate(hist.date.time.min=format(hist.date.time.base, format = "%Y-%m-%d: %H:%M")) %>% mutate(hist.time=format(hist.date.time.base, format = "%H")) %>% filter(V1 > 1541359887621) %>% select(hist.date, hist.time, hist.date.time, hist.date.time.min, hist.text=V2) %>% filter(hist.date != as.Date('1970-01-01', "%Y-%m-%d"))

hist.simple <- gsub(",","", history$hist.text)
hist.simple <- gsub("'","", hist.simple)
word.freq <- data.frame(table(unlist(strsplit(tolower(hist.simple), " ")))) %>% arrange(desc(Freq))

hist.count <- count(history, hist.date.time)
```

I ran a total of `r nrow(history)` lines of code, which contained `r sum(stri_count_words(history$hist.text))` words. I ran on average, `r summarise(hist.count, mean(n))` commands an hour, or `r summarise(hist.count, mean(n))/60` per minute, with a median of `r summarise(hist.count, median(n))/60` commands per minute and a maximum of `r summarise(hist.count, max(n))/60` per minute. As I was running a lot of for loops, this high rate isn't unexpected.

The most frequently run commands involved, as expected, assignment and piping:, <- at `r word.freq %>% filter(Var1=='<-') %>% select(Freq)` times, %>% at `r word.freq %>% filter(Var1=='%>%') %>% select(Freq)` times and = at `r word.freq %>% filter(Var1=='=') %>% select(Freq)` times. The most common R functions were distinct() at `r word.freq %>% filter(Var1=='distinct()') %>% select(Freq)` and nrow at `r word.freq %>% filter(Var1=='nrow') %>% select(Freq)`, which I found a bit surprising until I realised that I use nrow in the footfall loop to read in the data and I used distinct() in the bus route data, which I had a bit of trouble with. The most common variables were the column names I gave to the footfall columns such as mon.in, mon.out etc, at`r word.freq %>% filter(Var1=='fri.in') %>% select(Freq)` each. As these run inside a loop to read in all the footfall data, it's not surprising that these were the most common.

As I've been trying to get better at using the built in R help, I measured the number of times I used the help function by looking at the number of times '?' was used. I probably still have to get better at using it, as you can see from the figure below.

```{r}
hist.count.all <- history %>% count(hist.date.time) %>% mutate(label='All Commands')
hist.help <- history[which(grepl('\\?', history$hist.text)),] %>% count(hist.date.time) %>% mutate(label='Help Function Used')

ggplot(data=rbind(hist.help, hist.count.all), aes(x=hist.date.time, y=n)) + facet_grid(rows=vars(label), scale = 'free_y') + geom_histogram(stat = 'identity', fill='darkorchid3', colour='black') + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = 'Date and Hour', y = 'Number of Commands Run', title='Figure X: Number of Commands Run per Date and Hour')
```

I can see from the graph that I tended to work in large chunks, not taking many breaks. This isn't ideal for concentration, but as I have other commitments, it's the only way I can fit in the work.


## 5.2 Personal Reflection
1.)
challenges overcome (work-arounds), 2.) efficient work practices 3.) areas of
frustration 4.) time management. A useful way to structure a reflection is by
using the four I's e.g. Identify an element during the project (past) - results,
quality, emotions, productivity. Investigate - determine the source of this (present).
Incorporate - based on the investigation, is there a lesson to absorb (future)?
Improve - the goal of reflection.

The length of the project was the main challenge for me. Some of the parts were quite complex but I enjoyed figuring them out, my main issue was finding time to do everything.

I decided to approach the project by attempting each problem until I got stuck and then moving on to the next one if I was stuck for more than 20 minutes or so, and then going back to the start to finish the problems to the required standard. I've learned from previous experience that it's easier for me to work on improving a question rather than starting something from scratch. By looking at each question in some detail at the start, is also allows ideas to peculate, so when you come back to it later, you might have solved the problem or have a new perspective. When I actually stuck to this, it worked well, but a few times I kept at something when I just should have given up and moved on, at least for the moment. Towards the end of the assignment, I made a Trello to do list which had the 'minimum viable answer' for each task, so I would stick to doing that. I ca go back and improve it later.

I found some of the tasks challenging, like finding a dataset to compare to the footfall one, mainly because the footfall data was from 2013 and it was difficult to find an easily accessible dataset from that time.

 

It's also easy to get stuck down a rabbit hole of trying to solve a particular technical problem but then when you look at it with a fresh mind, you realised that problem didn't need to be solved at all for the question.

Got stuck down a rabbit hole of joining together dataframes in task 3 and got stuck.
Came back to it another day and reread the question and realised I was making it overly complicated.

I sometimes found it hard to figure out whether there was a small issue with my code and I was almost there, or if I was completely wrong: loading bike JSON.

As some of the questions were of the type 'find the most interesting information', I decided to first so something relatively basic and straightforward and then come back to it at a later date and improve it if I had time. This would also hopefully give me time to come up with a better idea.

Because we have a test coming up where we won't have internet access, I've started using the built in R help function instead of googling the same information, which I think it a better habit to get into as you get to understand the inner workings of the function instead of letting someone else interpret it for you.


[1] https://www.irishexaminer.com/ireland/fivefold-increase-in-online-shopping-since-2007-469059.html (original Euromonitor International report not freely available online)

[2] https://www.cso.ie/px/pxeirestat/Database/eirestat/Overseas%20Travel/Overseas%20Travel_statbank.asp?SP=Overseas%20Travel&Planguage=0y (report TMA14: Overseas Trips to and from Ireland by Trips and Year)

[3] https://developers.google.com/transit/gtfs/reference/

[4] https://data.smartdublin.ie/dataset/ambient-sound-monitoring-network