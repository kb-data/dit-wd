---
title: 'Working with Data Assignment 1'
author: 'Kate Byrne'
date: '31 October 2018'
output: html_document
---
### Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, include=TRUE, warning=FALSE, message=FALSE)
options(digits=2)
```

Required R libraries are below
```{r, message=FALSE}
library(devtools)
library(readr)
library(ggplot2)
library(ggmap)
devtools::install_github('hadley/ggplot2')
devtools::install_github('dkahle/ggmap')
library(rnoaa)
library(httr)
library(plyr)
library(jsonlite)
library(dplyr)
library(readODS)
library(knitr)
library(hms)
library(lubridate)
```


##Task 1: Quarterly Economic Indicators
###1.1 Dublin Employment Trends
Below is the plot of quarterly Dublin Employment Trends for the period 2006 - 2016.

```{r include=TRUE, results='hide', fig.height=5, fig.width=12}
employ.read <- read_delim('dublin employment trends.txt', ':')
qplot(data = employ.read , x = Time , y = Employment, geom = 'line', xlab = 'Quarterly Figures', ylab = 'Trend', main = 'Dublin Employment Trends Per Sector: 2006 - 2016') + facet_grid ( .~ Sector) 
```

From the graph above, and the table of maximum and minimum values below, it appears that construction, finance, retail and transport all largely follow the same pattern of peaking in the earlier time period (construction, finance and retail: late 2007/early 2008, transport, late 2010) and then decreasing. 

There is a sharp decrease in both construction and retail, with a less sharp drop in both finance and transport. This would be influenced by the nature of these these industries. In both retail and construction, people are employed to directly provide a product for sale, if sales are down, staff are let go. As many staff in these industries can also be on unsecure contracts, this can happen quite quickly. While the volumes of business and profits generated by both the finance and transport sectors was affected by the economic downturn (less people in work would mean less people getting peak time transport, less mortgages and loans were processed in the financial sector), a lot of their services are required regardless of economic factors (regular transportation, maintaining accounts and processing transactions), so while profits may have been down, the number of people employed could not decrease as quickly. Also, people in these industries can have more secure employment contracts so lay-offs might not happen as quickly. 

Of these industries, some either appear to recover fully (transport) or partially (finance, construction). However, retail seems to be contuining an overall downward trajectory. While all sectors would have been affected by the downturn in the economy, there was a 5-fold increase in online shopping in Ireland between 2007 and 2017 [1], which would affect the recovery of high street retail. Also, since many of the large online retailers are not based in Ireland (e.g. ASOS and Amazon are UK based), the shift to online shopping would further reduce the numbers of people employed in retail in Dublin and Ireland. 

Employment in IT have been increasing steadily since the end of 2010. This has been a growing industry in Ireland, and specifically Dublin due to the arrival of several international Tech companies such as AirBnb (2013) and Facebook (2008). 

As the number of tourists coming to Ireland and Dublin has increased in recent years [2], you would expect that the number of people employed in the tourism sector has increased. Since the recession between 2007 and 2009 was a global recession, you would expect the number of tourists coming to Ireland in this period would have been impacted. 

As employment in the scientific sector largely depends on government and indystry sponsorship and grants which are often for a fixed period of time such as 3-5 years, we can see why the time period immediately after the recession had the lowest rates, if less new grants were awarded during the recession period, this would impact the number of people employed in the following years.

Below are the dates for minumum and maximum employment values for each sector. Finance has three quarters with the same maximum.
```{r}
min.employ <- employ.read %>% group_by(Sector) %>% filter(., Employment == min(Employment))
max.employ <- employ.read %>% group_by(Sector) %>% filter(., Employment == max(Employment))
employ.extreme <- left_join(min.employ, max.employ, by = 'Sector', suffix = c('.min', '.max'))[c('Sector', 'QuarterYear.min', 'Employment.min', 'Time.min', 'QuarterYear.max', 'Employment.max', 'Time.max')]
employ.extreme <- employ.extreme %>% mutate(Employment.diff = Employment.max-Employment.min)
kable(employ.extreme, digits = 2)
```


###1.2 Dublin Property Trends

```{r}
prop.read <- read_tsv('dublin property trends.txt')
qplot(data = prop.read , x = Time , y = Trend, geom = 'line', colour = Category, xlab = 'Time', ylab = 'Trend', main = 'Dublin Property Trends: 2007 - 2016')
```

```{r}
prop.split <- split(prop.read, prop.read$Category)
prop.apt.rent <- select(prop.split[[1]], Time, Year, Trend, Category)
prop.house.price <- select(prop.split[[2]], Time, Year, Trend, Category)
prop.house.rent <- select(prop.split[[3]], Time, Year, Trend, Category)
prop.house.built <- select(prop.split[[4]], Time, Year, Trend, Category)

prop.cols <- prop.apt.rent %>% left_join(prop.house.price, by = c('Time', 'Year'), suffix = c('.apt.rent', '.house.price')) %>% left_join( prop.house.built, by = c('Time', 'Year'), suffix = c('.', '.house.built')) %>% left_join(., prop.house.rent, by = c('Time', 'Year'), suffix = c('.', '.house.rent')) 

prop.filter.buy <- filter(prop.cols, (Trend.house.price > Trend.house.rent & Trend.house.price > Trend.apt.rent & Trend. > Trend.house.rent & Trend. > Trend.apt.rent))
prop.filter.rent <- filter(prop.cols, (Trend.house.price < Trend.house.rent & Trend.house.price < Trend.apt.rent & Trend. < Trend.house.rent & Trend. < Trend.apt.rent))
```

The timeperiod in this dataset is from `r unique(filter(prop.read, Time == min(Time))$Year)` to `r unique(filter(prop.read, Time == max(Time))$Year)`. Over this time, the balance of rent vs buy has changed in Dublin. For the time period `r filter(prop.filter.buy, Time == min(Time))$Year` to `r filter(prop.filter.buy, Time == max(Time))$Year`, both house prices and houses built outstripped both houses and apartments rented. For the time period `r filter(prop.filter.rent, Time == min(Time))$Year` to `r filter(prop.filter.rent, Time == max(Time))$Year`, the situation is reversed and both house and apartment rental outstripped house prices and house buying.

Comparing this data to the construction employment figures in the previous question, we can see that the number of houses built and house prices behave in a similar pattern to the construction employment figures. The highest rates for house prices, houses built and construction jobs are at Q3 2007, Q3 2007 and Q4 2007 respectively. It should be noted that this is at the start of the dataset for house prices and houses being built so there may have been higher rates before this. The construction employmant data started in Q1 2006 so we can say that Q4 2007 is the local maximum rate. Similarly, the lowest rates for house prices, houses built and construction jobs are Q3 2012, Q2 2012 and Q1 2013 repectively.

```{r}
min.prop <- prop.read %>% group_by(Category) %>% filter(., Trend == min(Trend))
max.prop <- prop.read %>% group_by(Category) %>% filter(., Trend == max(Trend))
prop.extreme <- left_join(min.prop, max.prop, by = 'Category', suffix = c('.min', '.max'))[c('Category', 'Year.min', 'Trend.min', 'Time.min', 'Year.max', 'Trend.max', 'Time.max')]
prop.extreme <- prop.extreme %>% mutate(Trend.diff = Trend.max-Trend.min)
employ.extreme.con <- employ.extreme %>% filter(., Sector == 'Construction') %>% setNames(c('Category', 'Year.min', 'Trend.min', 'Time.min', 'Year.max',  'Trend.max', 'Time.max', 'Trend.diff'))
kable(bind_rows(prop.extreme, employ.extreme.con), digits = 2)
```


## Task 2: Real Time Bike Info

### 2.1 Summarise information available

```{r}
stations <- GET('https://api.jcdecaux.com/vls/v1/stations?apiKey=174980a157364085f718726daecb380b7aeaf5b4')
http_type(stations)
stations.df <- jsonlite::fromJSON(content(stations, 'text'), simplifyVector = TRUE)
#count(stations.df, vars = 'contract_name')
```

As Dublin has quite a high number of stations and I'm most familiar with Dublin, I'll choose that.

```{r}
bike <- GET('https://api.jcdecaux.com/vls/v1/stations?contract=Dublin&apiKey=174980a157364085f718726daecb380b7aeaf5b4')
http_type(bike)
bike.df <- jsonlite::fromJSON(content(bike, 'text'), simplifyVector = TRUE)
```
```{r}
sum.bike <- summary(bike.df)
```


There are a total of `r nrow(bike.df)` bike stands in the Dublin bike data, they are numbered `r bike.df[1,1]` to `r bike.df[nrow(bike.df),1]`, there is no bike stand number 20. There are `r ncol(bike.df)` columns in the dataset. 
Cannot extract column from dataframe!
In most cases the name and address are the same, except for - strip punctuation and match

All bikes are withing x of O'connel street bridge.

The number of available bikes and available bike stands are also available

The status of whether the bike stands are open or closed is also given. As of now(`r Sys.time()`), all bike stands are `r unique(bike.df$status)`.Payment is only available at some bike stands, 

How many NAs per column, is the dataset relatively complete?
Min max of each column, any weird data?
aNY COLUMNS NOT USEFUL? BONUS

Average number of bike stands
Biggest and smallest stations

Proportion that you can pay at
Are the paying ones bigger or smaller?

Average number of bikes and stands available

Visualise some data?
```{r}
#filter(bike.df, banking==FALSE)
#bike.df[bike.df$banking==F]

#bike.df[bike.df$address=='Smithfield North']

#bike.df$address=='Smithfield North'

#bike.df$banking
```


```{r}
bike.df <- bike.df[order(bike.df$number),] 
#bike.df
```

```{r}
str(bike.df)
```

### 2.2 Plot Dublin Bikes Data

If someone quickly needed to get a Dublin bike, but didn't have a Dublin bike card already set up, they would need a bike station with payment facilities. Below is a plot of the number of available bikes by station and whether the bike station takes payment.

```{r , fig.height=14, fig.width=12}
qplot(available_bikes, name, data = bike.df , colour = banking, size=I(3))
```

## Task 3 Dublin Bus
### 3.1 Shaping and describing the data

```{r}
unzip('DublinBus.zip')
trips.df <- read_csv('googletransitdublinbusp20130315-1546\\trips.txt')
routes.df <- read_csv('googletransitdublinbusp20130315-1546\\routes.txt')
calendar.df <- read_csv('googletransitdublinbusp20130315-1546\\calendar.txt')
calendar_dates.df <- read_csv('googletransitdublinbusp20130315-1546\\calendar_dates.txt')
shapes.df <- read_csv('googletransitdublinbusp20130315-1546\\shapes.txt')
stops.df <- read_csv('googletransitdublinbusp20130315-1546\\stops.txt')
transfers.df <- read_csv('googletransitdublinbusp20130315-1546\\transfers.txt')
agency.df <- read_csv('googletransitdublinbusp20130315-1546\\agency.txt')
stop_times.df <- read_csv('googletransitdublinbusp20130315-1546\\stop_times.txt') 
```

There are `r length(unique(routes.df$route_id))` unique routes in this dataset, all of which are route type `r unique(routes.df$route_type)`, 'Bus. Used for short- and long-distance bus routes.' [1]. There was a total of `r length(unique(trips.df$trip_id))` different possible trips. The number of daily journeys for each route, split out by service type (weekdays, Saturdays and Sunday/Bank Holiday) are shown in Figure X below.

```{r fig.width=14, fig.height=10}
route.num.trip <- left_join(trips.df, routes.df, by = 'route_id') %>% mutate(Service=case_when(service_id == 2 ~ 'Sunday/Bank Holiday', service_id == 1 ~ 'Weekday',  service_id == 3 ~ 'Saturday')) %>% select(Service, route_short_name, trip_id) %>% count(., Service, route_short_name)

ggplot(data=route.num.trip, aes(x=reorder(route_short_name, -n), y=n)) + facet_grid(rows=vars(Service), scale = 'fixed') + geom_histogram(stat = 'identity', fill='darkorchid3', colour='black') + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = 'Routes', y = 'Number of Trips', title='Figure X: Number of journeys on each route by service type')
```


```{r}
#common join used at multiple points in the code
bus.join <- left_join(routes.df, trips.df, by='route_id') %>% left_join(., stop_times.df, by='trip_id') %>% left_join(., stops.df, by='stop_id') %>% mutate(Service=case_when(service_id == 2 ~ 'Sunday/Bank Holiday', service_id == 1 ~ 'Weekday',  service_id == 3 ~ 'Saturday'))

#calculating the unit of distance in the dataset
dist.calc <- bus.join %>% filter(route_short_name == 16) %>% filter(trip_id == min(trip_id)) %>% filter(stop_name == 'Redmond\'s Hill') %>% select(shape_dist_traveled) - bus.join %>% filter(route_short_name == 16) %>% filter(trip_id == min(trip_id)) %>% filter(stop_name == 'George\'s St') %>% select(shape_dist_traveled)

#calculating the route lengths
route.length <- bus.join %>% group_by(route_short_name, route_long_name, trip_id) %>% summarise(Dist.all = max(shape_dist_traveled)) %>% ungroup() %>% group_by(route_short_name, route_long_name) %>% summarise(Dist.mean = mean(Dist.all)/1000) %>% ungroup()

# base df that most and least frequent routes and mean length is based on
bus.freq <- bus.join %>% group_by(route_short_name, service_id,direction_id) %>% filter(., stop_sequence==1) %>% select(., route_short_name, route_long_name, Service, direction_id, arrival_time, stop_sequence) %>% arrange(., route_short_name, Service, direction_id, arrival_time) %>% mutate(., Freq = difftime(arrival_time,lag(arrival_time),units='mins')) %>% na.omit() %>% filter(Freq>0)  %>% ungroup() %>% select(route_short_name, route_long_name, Service, Freq) %>% group_by(Service)

freq.mean <- bus.freq %>% summarise(Freq=mean(Freq))

most.freq <- bus.freq %>% filter(Freq==min(Freq)) %>% distinct() %>% mutate(Classification='Most Fequent') %>% ungroup()
leastfreq <- bus.freq %>% filter(Freq==max(Freq)) %>% distinct() %>% mutate(Classification='Least Fequent') %>% ungroup()

extreme.freq <- rbind(most.freq, leastfreq) %>% select(Service, Classification, Freq, Route=route_short_name, Route.name=route_long_name) %>% arrange(., Service, Freq) 
```

The mean frequencies across all routes for each type of service are as follows: `r slice(freq.mean,1)$Service`: `r slice(freq.mean,1)$Freq` mins, `r slice(freq.mean,2)$Service`: `r slice(freq.mean,2)$Freq` mins and `r slice(freq.mean,3)$Service`: `r slice(freq.mean,3)$Freq` mins. The routes with the highest and lowest frequencies routes for each type of service, along with their frequency in minutes are shown in the table below.

```{r}
kable(extreme.freq)
```

```{r}
##trying to figure out first and last bus - getting bogged down
#bus.join %>% mutate(Service=case_when(service_id == 2 ~ 'Sunday/Bank Holiday', service_id == 1 ~ 'Weekday',  service_id == 3 #~ 'Saturday')) %>% select(Service, route_short_name, route_long_name, arrival_time) %>% #mutate(actual.time=case_when(arrival_time < as.hms('04:00:00') ~ arrival_time+as.hms('24:00:00')))  %>% #filter(route_short_name == 41, Service=='Sunday/Bank Holiday') %>% arrange(arrival_time)

#library(hms)
#typeof(as.hms('04:00:00'))

#%>% group_by(Service) %>% filter(arrival_time==min(arrival_time, na.rm=TRUE) | arrival_time==max(arrival_time, na.rm=TRUE)) %>% select(Service, route_short_name, route_long_name, arrival_time)

#bus.join %>% mutate(Service=case_when(service_id == 2 ~ 'Sunday/Bank Holiday', service_id == 1 ~ 'Weekday',  service_id == 3 ~ 'Saturday'))  %>% filter(route_short_name == 41) %>% select(Service, route_short_name, service_id, arrival_time, departure_time)  %>% distinct() %>% arrange(arrival_time)
```


The route map is shown below in Figure X, overlaid over a map of Dublin. The longest and shortest routes are shown.
```{r fig.height=14, fig.width=14}
lat.long.all <- select(stops.df, Latitude=stop_lat, Longitude=stop_lon) %>% mutate(., Route='All Routes') %>% select(Route, Longitude, Latitude) %>% distinct()

lat.long.longest <- filter(bus.join, route_short_name==filter(route.length, Dist.mean==max(Dist.mean))$route_short_name) %>% select(., Route=route_short_name, Longitude=stop_lon, Latitude=stop_lat) %>% distinct()

lat.long.shortest <- filter(bus.join, route_short_name==filter(route.length, Dist.mean==min(Dist.mean))$route_short_name) %>% select(., Route=route_short_name, Longitude=stop_lon, Latitude=stop_lat) %>% distinct()

lat.long.combo <- rbind(lat.long.all, lat.long.longest, lat.long.shortest) 

# store bounding box coordinates - add a small amount so box extends slightly beyond the furthermost stop
dublin_bb <- c(left = (min(stops.df$stop_lon) - 0.01),
            bottom = (min(stops.df$stop_lat) - 0.01),
            right = (max(stops.df$stop_lon) + 0.01),
            top = (max(stops.df$stop_lat) + 0.01))

dublin_stamen <- get_stamenmap(bbox = dublin_bb, zoom = 11,  maptype = 'toner-lite')

ggmap(dublin_stamen, legend = 'right') + geom_point(data = lat.long.combo, mapping = aes(x = Longitude, y = Latitude, na.rm=TRUE, colour = Route), size = 0.6, alpha = .5) +ggtitle('Figure X: Map of Dublin Bus Routes') + labs(x='Longitude', y='Latitude')
```

The units of distance travelled are not specified in the dataset. The distance on the 16 route from the George's Street stop to the Redmond's Hill stop is given in the dataset is `r dist.calc`. The distance between these two stops according to Google maps is 500m, so it appears that the unit of distance is meters. The mean and median route length are very similar at `r mean(route.length$Dist.mean)` km and `r median(route.length$Dist.mean)` km respectively. The shortest route is Route `r filter(route.length, Dist.mean==min(Dist.mean))$route_short_name`, `r filter(route.length, Dist.mean==min(Dist.mean))$route_long_name`, at `r filter(route.length, Dist.mean==min(Dist.mean))$Dist.mean` km. The longest route is Route `r filter(route.length, Dist.mean==max(Dist.mean))$route_short_name`, `r filter(route.length, Dist.mean==max(Dist.mean))$route_long_name`, at `r filter(route.length, Dist.mean==max(Dist.mean))$Dist.mean` km. Both can ben seen in Figure X above.

### Pick a route
```{r}
home.route <- 49
bus.home <- bus.join %>% filter(route_short_name==home.route, Service=='Weekday')
```

I live in Harold's Cross and work in College Green so am picking Route `r home.route`, `r bus.home %>% select(route_long_name) %>% distinct()`. According to the data, direction_id = 1 is in the direction of `r bus.home %>% filter(direction_id==1) %>% select(stop_headsign) %>% distinct() `, so that's the direction I would get to work. Direction_id = 0 is in the direction of `r bus.home %>% filter(direction_id==0) %>% select(stop_headsign) %>% distinct()`, so that would be the direction I would get home from work. The stops in question are listed below.
```{r}
home.stops <- bus.home %>%  filter(stop_id=='8220DB001296' | stop_id == '8220DB004521' | stop_id=='8220DB001339' | stop_id=='8220DB005192')
kable(home.stops %>% select(route_short_name, route_long_name, direction_id, stop_headsign, stop_id, stop_name) %>% distinct() )
```

```{r}
commute.trips <- home.stops %>% filter((direction_id==1 & stop_id=='8220DB005192' & arrival_time>as.hms('08:15:00') & arrival_time<as.hms('09:00:00')) | (direction_id==0 & stop_id=='8220DB004521' & arrival_time>as.hms('17:00:00') & arrival_time<as.hms('17:45:00'))) %>% select(trip_id)

length.route <- home.stops %>% inner_join(commute.trips, by='trip_id') %>% arrange(trip_id, stop_sequence) %>% group_by(direction_id, trip_id) %>% mutate(length.time=(lead(arrival_time) - arrival_time)/60) %>% mutate(length.dist=(lead(shape_dist_traveled) - shape_dist_traveled)/1000) %>% ungroup() %>% select(direction_id, stop_headsign, length.time, length.dist) %>% filter(!is.na(length.dist)) %>% distinct()
```

The trip into work should take `r filter(length.route, direction_id==1)$length.time` mins and `r filter(length.route, direction_id==1)$length.dist` km. The trip home from work should take `r filter(length.route, direction_id==0)$length.time` mins and `r filter(length.route, direction_id==0)$length.dist` km. Neither of these times appear to take into consideration traffic though.

If I needed to arrive for work at between 8.15 and 9am, and want to get a bus home from work between 5pm and 5.45 pm, I'll have to get one of the services below. 

```{r}
kable(inner_join(home.stops, commute.trips, by='trip_id') %>% mutate(Journey=case_when(stop_headsign=='Pearse St' ~ 'Commute to work', stop_headsign=='The Square' ~ 'Commute to home') ) %>% select(Service, Journey, Direction=stop_headsign, stop_name, arrival_time))
```

The times for the earliest and latest busses on Route `r home.route` are below.
```{r}
kable(bus.join %>% group_by(Service, stop_headsign) %>% filter(!is.na(arrival_time), route_short_name==home.route) %>% summarise(earliest.bus=min(arrival_time), latest.bus= max(arrival_time)))
```



I like exploring Dublin on a Saturday but I don't have a car and I don't like walking between stops to change bus. Here I'll look at everywhere that I can get to, using busses that interconnect with my chosen route `r home.route`. This analysis can easily be changed to look at other routes by changing the bus route chosen above.
```{r}
bus.join.sat <- filter(bus.join, Service=='Saturday')
stops.home <- bus.join.sat %>% filter(route_short_name==home.route)

stops.intersect <- inner_join(stops.home, bus.join.sat, by='stop_id', suffix=c('.home', '.intersect')) %>% select(route_short_name.intersect) %>% distinct() %>% inner_join(bus.join.sat, by = c("route_short_name.intersect" = "route_short_name")) %>% select(Longitude=stop_lon, Latitude=stop_lat) %>% distinct()

intersect.routes <- rbind((stops.intersect %>% mutate(Route=paste('All Routes Intersecting with Route', home.route))), (stops.home %>% select(Longitude=stop_lon, Latitude=stop_lat) %>% mutate(Route=paste('Route', home.route)))) %>% select(Route, Longitude, Latitude)
```

```{r fig.height=14, fig.width=14}
# store bounding box coordinates - add a small amount so box extends slightly beyond the furthermost stop
dublin_intersect_bb <- c(left = (min(stops.intersect$Longitude) - 0.01),
            bottom = (min(stops.intersect$Latitude) - 0.01),
            right = (max(stops.intersect$Longitude) + 0.01),
            top = (max(stops.intersect$Latitude) + 0.01))

dublin_intersect_stamen <- get_stamenmap(bbox = dublin_intersect_bb, zoom = 11,  maptype = 'toner-lite')

ggmap(dublin_intersect_stamen, legend = 'right') + geom_point(data = intersect.routes, mapping = aes(x = Longitude, y = Latitude, na.rm=TRUE, colour = Route), size = 0.6, alpha = .5) +ggtitle('Figure X: Map of Dublin Bus Routes') + labs(x='Longitude', y='Latitude')
```



# Task 4
```{r}
#loading in the footfall data
url.footfall <- 'https://data.smartdublin.ie/dataset/8204be0a-6348-459e-96e9-65bb75600ec3/resource/384fe47a-2f25-4f52-8fc5-8e61899951e9/download/pedestrianfootfall2013.ods'
download.file(url.footfall, 'footfall_ods.ods')
sheet.names <- ods_sheets('pedestrianfootfall2013.ods')
```
```{r}
#getting list of camera locations
camera.locations <- read_ods('pedestrianfootfall2013.ods',sheet.names[1])[1]
colnames(camera.locations) <- c('Loc') 
camera.locations %>% filter(., grepl("Entrance Name:", Loc, fixed = TRUE)) 
```

```{r}
sheet.names
```


```{r}
#
camera1.name <- 'Entrance Name: Capel St at Mullen'
camera2.name <- 'Entrance Name: Grafton St at M&S'

camera1.all <- data.frame(matrix(ncol = 15, nrow = 0))
colnames(camera1.all) <- c('Time', 'Mon.In', 'Mon.Out', 'Tue.In', 'Tue.Out', 'Wed.In', 'Wed.Out', 'Thurs.In', 'Thurs.Out', 'Fri.In', 'Fri.Out', 'Sat.In', 'Sat.Out', 'Sun.In', 'Sun.Out')

camera2.all <- data.frame(matrix(ncol = 15, nrow = 0))
colnames(camera2.all) <- c('Time', 'Mon.In', 'Mon.Out', 'Tue.In', 'Tue.Out', 'Wed.In', 'Wed.Out', 'Thurs.In', 'Thurs.Out', 'Fri.In', 'Fri.Out', 'Sat.In', 'Sat.Out', 'Sun.In', 'Sun.Out')

for(week.i in 1:length(sheet.names))
{
  f1 <- read_ods('pedestrianfootfall2013.ods',sheet.names[week.i])
  
  if (ncol(f1)==15)
  {
    colnames(f1) <- c('Time', 'Mon.In', 'Mon.Out', 'Tue.In', 'Tue.Out', 'Wed.In', 'Wed.Out', 'Thurs.In', 'Thurs.Out', 'Fri.In', 'Fri.Out', 'Sat.In', 'Sat.Out', 'Sun.In', 'Sun.Out')
  } else {
    colnames(f1) <- c('Time', 'Mon.In', 'Mon.Out', 'Tue.In', 'Tue.Out', 'Wed.In', 'Wed.Out', 'Thurs.In', 'Thurs.Out', 'Fri.In', 'Fri.Out', 'Sat.In', 'Sat.Out', 'Sun.In', 'Sun.Out', 'unknown1', 'unknown2', 'unknown3')
  }
  f1 <- select(f1, Time, Mon.In, Mon.Out, Tue.In, Tue.Out, Wed.In, Wed.Out, Thurs.In, Thurs.Out, Fri.In, Fri.Out, Sat.In, Sat.Out, Sun.In, Sun.Out)
  
  camera1.row <- which(grepl(camera1.name, f1$Time))
  camera2.row <- which(grepl(camera2.name, f1$Time))

  camera1.slice <- slice(f1, (camera1.row+3):(camera1.row+26)) %>% mutate(Week=sheet.names[week.i])
  camera2.slice <- slice(f1, (camera2.row+3):(camera2.row+26)) %>% mutate(Week=sheet.names[week.i])
  
  camera1.all <- rbind(camera1.all, camera1.slice)
  camera2.all <- rbind(camera2.all, camera2.slice)
  
  f1 <- NULL
}
```

```{r}
#all fields are character (they contained some text before using split), so must change all numeric characters to numeric
camera1.all$Mon.In <- as.numeric(camera1.all$Mon.In)
camera1.all$Mon.Out <- as.numeric(camera1.all$Mon.Out)
camera1.all$Tue.In <- as.numeric(camera1.all$Tue.In)
camera1.all$Tue.Out <- as.numeric(camera1.all$Tue.Out)
camera1.all$Wed.In <- as.numeric(camera1.all$Wed.In)
camera1.all$Wed.Out <- as.numeric(camera1.all$Wed.Out)
camera1.all$Thurs.In <- as.numeric(camera1.all$Thurs.In)
camera1.all$Thurs.Out <- as.numeric(camera1.all$Thurs.Out)
camera1.all$Fri.In <- as.numeric(camera1.all$Fri.In)
camera1.all$Fri.Out <- as.numeric(camera1.all$Fri.Out)
camera1.all$Sat.In <- as.numeric(camera1.all$Sat.In)
camera1.all$Sat.Out <- as.numeric(camera1.all$Sat.Out)
camera1.all$Sun.In <- as.numeric(camera1.all$Sun.In)
camera1.all$Sun.Out <- as.numeric(camera1.all$Sun.Out)

camera2.all$Mon.In <- as.numeric(camera2.all$Mon.In)
camera2.all$Mon.Out <- as.numeric(camera2.all$Mon.Out)
camera2.all$Tue.In <- as.numeric(camera2.all$Tue.In)
camera2.all$Tue.Out <- as.numeric(camera2.all$Tue.Out)
camera2.all$Wed.In <- as.numeric(camera2.all$Wed.In)
camera2.all$Wed.Out <- as.numeric(camera2.all$Wed.Out)
camera2.all$Thurs.In <- as.numeric(camera2.all$Thurs.In)
camera2.all$Thurs.Out <- as.numeric(camera2.all$Thurs.Out)
camera2.all$Fri.In <- as.numeric(camera2.all$Fri.In)
camera2.all$Fri.Out <- as.numeric(camera2.all$Fri.Out)
camera2.all$Sat.In <- as.numeric(camera2.all$Sat.In)
camera2.all$Sat.Out <- as.numeric(camera2.all$Sat.Out)
camera2.all$Sun.In <- as.numeric(camera2.all$Sun.In)
camera2.all$Sun.Out <- as.numeric(camera2.all$Sun.Out)

# there was inconsistency in sheet naming so fixing this
camera1.all$Week <- gsub('Sheet13', 'Week_13', camera1.all$Week)
camera2.all$Week <- gsub('Sheet13', 'Week_13', camera2.all$Week)

camera1.all$Week <- gsub('Week_1\\>', 'Week_01', camera1.all$Week)
camera1.all$Week <- gsub('Week_2\\>', 'Week_02', camera1.all$Week)
camera1.all$Week <- gsub('Week_3\\>', 'Week_03', camera1.all$Week)
camera1.all$Week <- gsub('Week_4\\>', 'Week_04', camera1.all$Week)
camera1.all$Week <- gsub('Week_5\\>', 'Week_05', camera1.all$Week)
camera1.all$Week <- gsub('Week_6\\>', 'Week_06', camera1.all$Week)
camera1.all$Week <- gsub('Week_7\\>', 'Week_07', camera1.all$Week)
camera1.all$Week <- gsub('Week_8\\>', 'Week_08', camera1.all$Week)
camera1.all$Week <- gsub('Week_9\\>', 'Week_09', camera1.all$Week)
camera2.all$Week <- gsub('Week_1\\>', 'Week_01', camera2.all$Week)
camera2.all$Week <- gsub('Week_2\\>', 'Week_02', camera2.all$Week)
camera2.all$Week <- gsub('Week_3\\>', 'Week_03', camera2.all$Week)
camera2.all$Week <- gsub('Week_4\\>', 'Week_04', camera2.all$Week)
camera2.all$Week <- gsub('Week_5\\>', 'Week_05', camera2.all$Week)
camera2.all$Week <- gsub('Week_6\\>', 'Week_06', camera2.all$Week)
camera2.all$Week <- gsub('Week_7\\>', 'Week_07', camera2.all$Week)
camera2.all$Week <- gsub('Week_8\\>', 'Week_08', camera2.all$Week)
camera2.all$Week <- gsub('Week_9\\>', 'Week_09', camera2.all$Week)

camera1.all
```

```{r}
summary(camera1.all)
summary(camera2.all)
```




```{r}
camera1.sum.week <- camera1.all %>% group_by(Week) %>% summarise_if(., is.numeric, sum)
camera2.sum.week <- camera2.all %>% group_by(Week) %>% summarise_if(., is.numeric, sum)
camera1.sum.week
camera1.sum.week[2:15]
rowSums(camera1.sum.week[2:15], na.rm = TRUE)
sum(camera1.sum.week[2:15])

camera1.sum.week %>% group_by(Week) %>% mutate(x=sum())
```


### Task 2.2
I chose an ambient sound dataset
```{r}
ambient.sound.url <- 'https://data.smartdublin.ie/dataset/a52fbbe2-1bff-4897-84af-34945f6fc8de/resource/30cb9275-f9fb-432a-af48-0a5f7ab462a4/download/chancerypark2013.zip'
download.file(ambient.sound.url, destfile = 'ambient_sound.zip')
unzip('ambient_sound.zip', exdir = './ambient_sound')
```

```{r}
# I need to loop through all dates in the year to pull out each text file. Here I create a df of all required components
months <- format(ISOdate(2013,1:12,1),"%B")

dates.df <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(dates.df) <- c('year', 'month.char', 'month.num', 'day')

for (i in 1:12)
{
  date.i <- data.frame(matrix(ncol = 4, nrow = 1))
  colnames(date.i) <- c('year', 'month.char', 'month.num', 'day')
  
  date.i$year <- '2013'
  date.i$month.char <- months[i]
  if(nchar(as.character(i))==1)
  {
    date.i$month.num <- paste('0', i, sep='')
  } else {
    date.i$month.num <- i
  }
  if(months[i]=='December')
  {
   date.i$day = 28 #the last 3 days of the year are excluded from the footfall dataset
  } else {
  date.i$day <- as.Date(paste('2013','-', i ,'-01', sep=''), '%Y-%m-%d') %>% days_in_month()
  }
  dates.df <- rbind(dates.df, date.i)
  
  date.i <- NULL
}

```

```{r}
sound.all <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(sound.all) <- c('week', 'day.week', 'hour', 'mean.noise.level')

for (i.month in 1:nrow(dates.df))
{
  for (i.day in 1:dates.df$day[i.month])
  {
    
    if(nchar(as.character(i.day))==1)
     {
      sound.file <- paste('ambient_sound/',dates.df$year[i.month], '/', dates.df$month.char[i.month], '/DCC-NOISE-001011-2013-', dates.df$month.num[i.month], '-0', i.day, 'T00-00-00.txt', sep='')
    } else {
     sound.file <- paste('ambient_sound/2013/', dates.df$month.char[i.month], '/DCC-NOISE-001011-2013-', dates.df$month.num[i.month], '-', i.day, 'T00-00-00.txt', sep='')
    }
    if (!file.exists(sound.file)) next

    sound.read <- read.table(sound.file) 
    sound.df <- sound.read %>% select(date.sound=V1, time=V2, noise.level=V3) 

    sound.all <- rbind.data.frame(sound.all, sound.df)
    
    sound.df<- NULL
  }
}
```


```{r}
sound.tidy <- sound.all
sound.tidy$time <-  gsub(",","", sound.tidy$time)
sound.tidy$noise.level <-  as.numeric(gsub(",","", sound.tidy$noise.level))
sound.tidy$date.sound <-  as.Date(sound.tidy$date.sound, format='%d/%m/%Y')

sound.tidy$hour <- format(strptime(sound.tidy$time,format='%H:%M:%S'), "%k")
sound.tidy.summary <- sound.tidy %>% group_by(date.sound, hour) %>% summarise(mean.noise.level=mean(noise.level)) 

sound.tidy.summary$Week <- paste('Week', strftime(sound.tidy.summary$date.sound, "%V"), sep='_')
sound.tidy.summary$day.week <- format(as.Date(sound.tidy.summary$date.sound), "%A") 

sound.tidy.all <- sound.tidy.summary[2:5] 
```


```{r}
sound.split <- split(sound.tidy.all, sound.tidy.all$day.week)

sound.mon <- sound.split$Monday %>% select(., Week, hour, mon.noise=mean.noise.level)
sound.tue <- sound.split$Tuesday %>% select(Week, hour, tue.noise=mean.noise.level)
sound.wed <- sound.split$Wednesday %>% select(Week, hour, wed.noise=mean.noise.level)
sound.thurs <- sound.split$Thursday %>% select(Week, hour, thurs.noise=mean.noise.level)
sound.fri <- sound.split$Friday %>% select(Week, hour, fri.noise=mean.noise.level)
sound.sat <- sound.split$Saturday %>% select(Week, hour, sat.noise=mean.noise.level)
sound.sun <- sound.split$Sunday %>% select(Week, hour, sun.noise=mean.noise.level)

sound.new <- sound.mon %>% full_join(sound.tue, by=c('Week', 'hour')) %>% full_join(sound.wed, by=c('Week', 'hour')) %>% full_join(sound.thurs, by=c('Week', 'hour')) %>% full_join(sound.fri, by=c('Week', 'hour')) %>% full_join(sound.sat, by=c('Week', 'hour')) %>% full_join(sound.sun, by=c('Week', 'hour')) %>% arrange(Week, hour)

sound.new 
```





# 5 Reflection
## 5.1 Analytic
```{r}
#The history required spans multiple files so will add all here and remove duplicates and rows about other subjects
file.list <- list('history_database/history_database', 'history_database/history_database_20181114_end',  'history_database/history_database_20181105_end', 'history_database/history_database_20181103','history_database/history_database_20181031')

history.all <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(sound.all) <- c('date.mili', 'code')

for (i.hist in 1:length(file.list))
{
  history.read <- read.delim(file.list[[i.hist]], header=FALSE, sep=':', stringsAsFactors=FALSE) 
  history.all <- rbind.data.frame(history.all, history.read)
  
  history.read<- NULL
}

history <- history.all %>% distinct() %>% filter(V1 > 1541359887621) %>% mutate(hist.date=as.POSIXct(as.numeric(V1)/1000, origin=as.Date('1970-01-01', "%Y-%m-%d"), format = "%Y-%m-%d %H:%M:%OS")) %>% select(hist.date, hist.text=V2) 
history
```


```{r}
rquery.wordcloud(x, type=c("text", "url", "file"), 
        lang="english", excludeWords = NULL, 
        textStemming = FALSE,  colorPalette="Dark2",
        max.words=200)
```



## 5.2 Personal Reflection
1.)
challenges overcome (work-arounds), 2.) efficient work practices 3.) areas of
frustration 4.) time management. A useful way to structure a reflection is by
using the four I's e.g. Identify an element during the project (past) - results,
quality, emotions, productivity. Investigate - determine the source of this (present).
Incorporate - based on the investigation, is there a lesson to absorb (future)?
Improve - the goal of reflection.

I decided to approach the project by attempting each problem until I got stuck and then moving on to the next one if I was stuck for more than 20 minutes or so, and then going back to the start to finish the problems to the required standard. I've learned from previous experience that it's easier for me to work on improving a question rather than starting something from scratch. By looking at each question in some detail at the start, is also allows ideas to perculate, so when you come back to it later, you might ahve solved the problem or have a new perspective. 

It's also easy to get stuck down a rabbit hole of trying to solve a particular technical problem but then when you looka t it with a fresh mind, you realised that problem didn't need to be solved at all for the question.

Got stuck down a rabbit hole of joining together dataframes in task 3 and got stuck.
Came back to it another day and reread the question and realised I was making it overly complicated.

I sometimes found it hard to figure out whether there was a small issue with my code and I was almost there, or if I was completely wrong: loading bike JSON.

As some of the questions were of the type 'find the most interesting information', I decided to first so something relatively basic and straightforward and then come back to it at a later date and improve it if I had time. This would also hopefully give me time to come up with a better idea.

BEcause we have a test coming up where we won't have internet access, I've started using the built in R help function instead of googling the same information, whichi I think it a better habit to get into as you get to understand the inner workings of the function instead of letting someone else intrepret it for you.


[1] https://www.irishexaminer.com/ireland/fivefold-increase-in-online-shopping-since-2007-469059.html (original Euromonitor International report not freely available online)

[2] https://www.cso.ie/px/pxeirestat/Database/eirestat/Overseas%20Travel/Overseas%20Travel_statbank.asp?SP=Overseas%20Travel&Planguage=0y (report TMA14: Overseas Trips to and from Ireland by Trips and Year)

[1] https://developers.google.com/transit/gtfs/reference/