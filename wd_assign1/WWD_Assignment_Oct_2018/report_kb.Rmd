---
title: 'Working with Data Assignment 1'
author: 'Kate Byrne'
date: '31 October 2018'
output: html_document
---
### Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, include=TRUE, warning=FALSE, message=FALSE)
options(digits=2)
```

Required R libraries are below
```{r, message=FALSE}
library(devtools)
library(readr)
library(ggplot2)
library(ggmap)
devtools::install_github('hadley/ggplot2')
devtools::install_github('dkahle/ggmap')
#library(rnoaa)
library(httr)
library(plyr)
library(jsonlite)
library(dplyr)
library(readODS)
library(knitr)
library(hms)
library(lubridate)
library(tidyr)
library(stringi)
```


##Task 1: Quarterly Economic Indicators
###1.1 Dublin Employment Trends
Below is the plot of quarterly Dublin Employment Trends for the period 2006 - 2016.

```{r include=TRUE, results='hide', fig.height=5, fig.width=12}
employ.read <- read_delim('dublin employment trends.txt', ':')
qplot(data = employ.read , x = Time , y = Employment, geom = 'line', xlab = 'Quarterly Figures', ylab = 'Trend', main = 'Dublin Employment Trends Per Sector: 2006 - 2016') + facet_grid ( .~ Sector) 
```

From the graph above, and the table of maximum and minimum values below, it appears that construction, finance, retail and transport all largely follow the same pattern of peaking in the earlier time period (construction, finance and retail: late 2007/early 2008, transport, late 2010) and then decreasing. 

There is a sharp decrease in both construction and retail, with a less sharp drop in both finance and transport. This would be influenced by the nature of these these industries. In both retail and construction, people are employed to directly provide a product for sale, if sales are down, staff are let go. As many staff in these industries can also be on unsecure contracts, this can happen quite quickly. While the volumes of business and profits generated by both the finance and transport sectors was affected by the economic downturn (less people in work would mean less people getting peak time transport, less mortgages and loans were processed in the financial sector), a lot of their services are required regardless of economic factors (regular transportation, maintaining accounts and processing transactions), so while profits may have been down, the number of people employed could not decrease as quickly. Also, people in these industries can have more secure employment contracts so lay-offs might not happen as quickly. 

Of these industries, some either appear to recover fully (transport) or partially (finance, construction). However, retail seems to be contuining an overall downward trajectory. While all sectors would have been affected by the downturn in the economy, there was a 5-fold increase in online shopping in Ireland between 2007 and 2017 [1], which would affect the recovery of high street retail. Also, since many of the large online retailers are not based in Ireland (e.g. ASOS and Amazon are UK based), the shift to online shopping would further reduce the numbers of people employed in retail in Dublin and Ireland. 

Employment in IT have been increasing steadily since the end of 2010. This has been a growing industry in Ireland, and specifically Dublin due to the arrival of several international Tech companies such as AirBnb (2013) and Facebook (2008). 

As the number of tourists coming to Ireland and Dublin has increased in recent years [2], you would expect that the number of people employed in the tourism sector has increased. Since the recession between 2007 and 2009 was a global recession, you would expect the number of tourists coming to Ireland in this period would have been impacted. 

As employment in the scientific sector largely depends on government and indystry sponsorship and grants which are often for a fixed period of time such as 3-5 years, we can see why the time period immediately after the recession had the lowest rates, if less new grants were awarded during the recession period, this would impact the number of people employed in the following years.

Below are the dates for minumum and maximum employment values for each sector. Finance has three quarters with the same maximum.
```{r}
min.employ <- employ.read %>% group_by(Sector) %>% filter(., Employment == min(Employment))
max.employ <- employ.read %>% group_by(Sector) %>% filter(., Employment == max(Employment))
employ.extreme <- left_join(min.employ, max.employ, by = 'Sector', suffix = c('.min', '.max'))[c('Sector', 'QuarterYear.min', 'Employment.min', 'Time.min', 'QuarterYear.max', 'Employment.max', 'Time.max')]
employ.extreme <- employ.extreme %>% mutate(Employment.diff = Employment.max-Employment.min)
kable(employ.extreme, digits = 2)
```


###1.2 Dublin Property Trends

```{r}
prop.read <- read_tsv('dublin property trends.txt')
qplot(data = prop.read , x = Time , y = Trend, geom = 'line', colour = Category, xlab = 'Time', ylab = 'Trend', main = 'Dublin Property Trends: 2007 - 2016')
```

```{r}
prop.split <- split(prop.read, prop.read$Category)
prop.apt.rent <- select(prop.split[[1]], Time, Year, Trend, Category)
prop.house.price <- select(prop.split[[2]], Time, Year, Trend, Category)
prop.house.rent <- select(prop.split[[3]], Time, Year, Trend, Category)
prop.house.built <- select(prop.split[[4]], Time, Year, Trend, Category)

prop.cols <- prop.apt.rent %>% left_join(prop.house.price, by = c('Time', 'Year'), suffix = c('.apt.rent', '.house.price')) %>% left_join( prop.house.built, by = c('Time', 'Year'), suffix = c('.', '.house.built')) %>% left_join(., prop.house.rent, by = c('Time', 'Year'), suffix = c('.', '.house.rent')) 

prop.filter.buy <- filter(prop.cols, (Trend.house.price > Trend.house.rent & Trend.house.price > Trend.apt.rent & Trend. > Trend.house.rent & Trend. > Trend.apt.rent))
prop.filter.rent <- filter(prop.cols, (Trend.house.price < Trend.house.rent & Trend.house.price < Trend.apt.rent & Trend. < Trend.house.rent & Trend. < Trend.apt.rent))
```

The timeperiod in this dataset is from `r unique(filter(prop.read, Time == min(Time))$Year)` to `r unique(filter(prop.read, Time == max(Time))$Year)`. Over this time, the balance of rent vs buy has changed in Dublin. For the time period `r filter(prop.filter.buy, Time == min(Time))$Year` to `r filter(prop.filter.buy, Time == max(Time))$Year`, both house prices and houses built outstripped both houses and apartments rented. For the time period `r filter(prop.filter.rent, Time == min(Time))$Year` to `r filter(prop.filter.rent, Time == max(Time))$Year`, the situation is reversed and both house and apartment rental outstripped house prices and house buying.

Comparing this data to the construction employment figures in the previous question, we can see that the number of houses built and house prices behave in a similar pattern to the construction employment figures. The highest rates for house prices, houses built and construction jobs are at Q3 2007, Q3 2007 and Q4 2007 respectively. It should be noted that this is at the start of the dataset for house prices and houses being built so there may have been higher rates before this. The construction employmant data started in Q1 2006 so we can say that Q4 2007 is the local maximum rate. Similarly, the lowest rates for house prices, houses built and construction jobs are Q3 2012, Q2 2012 and Q1 2013 repectively.

```{r}
min.prop <- prop.read %>% group_by(Category) %>% filter(., Trend == min(Trend))
max.prop <- prop.read %>% group_by(Category) %>% filter(., Trend == max(Trend))
prop.extreme <- left_join(min.prop, max.prop, by = 'Category', suffix = c('.min', '.max'))[c('Category', 'Year.min', 'Trend.min', 'Time.min', 'Year.max', 'Trend.max', 'Time.max')]
prop.extreme <- prop.extreme %>% mutate(Trend.diff = Trend.max-Trend.min)
employ.extreme.con <- employ.extreme %>% filter(., Sector == 'Construction') %>% setNames(c('Category', 'Year.min', 'Trend.min', 'Time.min', 'Year.max',  'Trend.max', 'Time.max', 'Trend.diff'))
kable(bind_rows(prop.extreme, employ.extreme.con), digits = 2)
```


## Task 2: Real Time Bike Info

### 2.1 Summarise information available

```{r}
# Getting list of available cities
stations <- GET('https://api.jcdecaux.com/vls/v1/stations?apiKey=174980a157364085f718726daecb380b7aeaf5b4')
http_type(stations)
stations.df <- jsonlite::fromJSON(content(stations, 'text'), simplifyVector = TRUE)
#stations.df %>% count(contract_name) %>% arrange(desc(n))
```

As Dublin has quite a high number of stations and I'm most familiar with Dublin so I'll choose that.

```{r}
#Pulling in the Dublin bikes data
bike <- GET('https://api.jcdecaux.com/vls/v1/stations?contract=Dublin&apiKey=174980a157364085f718726daecb380b7aeaf5b4')
http_type(bike)
bike.df <- jsonlite::fromJSON(content(bike, 'text'), simplifyVector = TRUE)
```

```{r}
# Creating some aggregated dataframes for brevity in inline code
bike.df.match <- bike.df[tolower(bike.df$name) != tolower(bike.df$address), ]
avail.bikes <- rbind(bike.df.clean %>% select(address, Num_bikes=available_bike_stands) %>% mutate(Bike_label = 'Available Bike Stands'), bike.df.clean %>% select(address, Num_bikes=available_bikes) %>% mutate(Bike_label = 'Available Bikes'))
```

At the time that the data was pulled via the API, `r Sys.time()`, all `r nrow(bike.df)` Dublin bike stands were `r count(bike.df, status)[1]`. The bike stands are numbered `r min(bike.df$number)` to `r max(bike.df$number)`, there is no bike stand number 20. 

People generally access Dublin bikes through a subscription service. For ???25 per year, you get a card that allows you unlimited bike rides of 30mins or less. It is possible to buy a 3 day pass with a credit card but this isn't available at the majority of stations. There are payment facilities available at `r count(bike.df, banking) %>% filter(banking==TRUE) %>% select(n)` of the `r nrow(bike.df)` bike stations.

There are `r ncol(bike.df)` columns in the dataset but several are redundant. As we used contract name to chose the data from Dublin, this column doesn't give us any additional information. The name and address columns are identical in most cases. There are 3 cases where they differ but these are either due to spelling or punctuation differences, name:`r bike.df.match[bike.df.match$number==50,]$name`, address: `rbike.df.match[bike.df.match$number==50,]$address` and name:`r bike.df.match[bike.df.match$number==58,]$name`, address: `rbike.df.match[bike.df.match$number==58,]$address`. Bike stand number 5 is at an intersection and the name, `r bike.df.match[bike.df.match$number==5,]$name` and address: `r bike.df.match[bike.df.match$number==5,]$address` are each road in the intersection. Therefore for our purposes, we will just use the address field as it's more readable. The bonus field also doesn't appear to add any information, it is `r bike.df$bonus[1]`in every case. The latitud and longitude of each station is also provided. Aside from the addition of some unneeded fields, the dataset is complete, there are no NAs in fields of interest.

The number of bike stands per station vary between`r min(bike.df$bike_stands)` and `r max(bike.df$bike_stands)`, with an average of `r mean(bike.df$bike_stands)`. At this point in time, there are `r bike.df.clean %>% filter(available_bikes==0) %>% nrow()` stations with no available bikes. There are `r bike.df.clean %>% filter(available_bike_stands==0) %>% nrow()` stations with no free bike stands, so someone would be unable to return a bike they had taken out. There is an average of `r bike.df.clean %>% summarise(mean(available_bikes))` available bikes and `r bike.df.clean %>% summarise(mean(available_bike_stands))` available bike stands per station. The number of available bikes and stands for each station can be seen in the figure below. As the total number of bike stands is the sum of the available bike and available stands, the size of the station can also be seen in the figure below.

```{r fig.height=14, fig.width=12}
ggplot(avail.bikes, aes(fill=Bike_label, y=Num_bikes, x=address)) + geom_bar(stat="identity") +
coord_flip() + labs(x = 'Stations', y = 'Number of available bikes or stands', title='Figure X: Availabiliy of bikes or stands at Dublin bike stations')
```


### 2.2 Plot Dublin Bikes Data

If someone quickly needed to get a Dublin bike, but didn't have a Dublin bike card already set up, they would need a bike station with payment facilities in order to buy a 3 day pass. Below is a plot of the number of available bikes by station and whether the bike station takes payment.

```{r , fig.height=14, fig.width=12}
qplot(available_bikes, address, data = bike.df , colour = banking, size=I(3))
```

## Task 3 Dublin Bus
### 3.1 Shaping and describing the data

```{r}
unzip('DublinBus.zip')
trips.df <- read_csv('googletransitdublinbusp20130315-1546\\trips.txt')
routes.df <- read_csv('googletransitdublinbusp20130315-1546\\routes.txt')
calendar.df <- read_csv('googletransitdublinbusp20130315-1546\\calendar.txt')
calendar_dates.df <- read_csv('googletransitdublinbusp20130315-1546\\calendar_dates.txt')
shapes.df <- read_csv('googletransitdublinbusp20130315-1546\\shapes.txt')
stops.df <- read_csv('googletransitdublinbusp20130315-1546\\stops.txt')
transfers.df <- read_csv('googletransitdublinbusp20130315-1546\\transfers.txt')
agency.df <- read_csv('googletransitdublinbusp20130315-1546\\agency.txt')
stop_times.df <- read_csv('googletransitdublinbusp20130315-1546\\stop_times.txt') 
```

There are `r length(unique(routes.df$route_id))` unique routes in this dataset, all of which are route type `r unique(routes.df$route_type)`, 'Bus. Used for short- and long-distance bus routes.' [1]. There was a total of `r length(unique(trips.df$trip_id))` different possible trips. The number of daily journeys for each route, split out by service type (weekdays, Saturdays and Sunday/Bank Holiday) are shown in Figure X below.

```{r fig.width=14, fig.height=10}
route.num.trip <- left_join(trips.df, routes.df, by = 'route_id') %>% mutate(Service=case_when(service_id == 2 ~ 'Sunday/Bank Holiday', service_id == 1 ~ 'Weekday',  service_id == 3 ~ 'Saturday')) %>% select(Service, route_short_name, trip_id) %>% count(., Service, route_short_name)

ggplot(data=route.num.trip, aes(x=reorder(route_short_name, -n), y=n)) + facet_grid(rows=vars(Service), scale = 'fixed') + geom_histogram(stat = 'identity', fill='darkorchid3', colour='black', alpha=0.7) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = 'Routes', y = 'Number of Trips', title='Figure X: Number of journeys on each route by service type')
```


```{r}
#common join used at multiple points in the code
bus.join <- left_join(routes.df, trips.df, by='route_id') %>% left_join(., stop_times.df, by='trip_id') %>% left_join(., stops.df, by='stop_id') %>% mutate(Service=case_when(service_id == 2 ~ 'Sunday/Bank Holiday', service_id == 1 ~ 'Weekday',  service_id == 3 ~ 'Saturday'))

#calculating the unit of distance in the dataset
dist.calc <- bus.join %>% filter(route_short_name == 16) %>% filter(trip_id == min(trip_id)) %>% filter(stop_name == 'Redmond\'s Hill') %>% select(shape_dist_traveled) - bus.join %>% filter(route_short_name == 16) %>% filter(trip_id == min(trip_id)) %>% filter(stop_name == 'George\'s St') %>% select(shape_dist_traveled)

#calculating the route lengths
route.length <- bus.join %>% group_by(route_short_name, route_long_name, trip_id) %>% summarise(Dist.all = max(shape_dist_traveled)) %>% ungroup() %>% group_by(route_short_name, route_long_name) %>% summarise(Dist.mean = mean(Dist.all)/1000) %>% ungroup()

# base df that most and least frequent routes and mean length is based on
bus.freq <- bus.join %>% group_by(route_short_name, service_id,direction_id) %>% filter(., stop_sequence==1) %>% select(., route_short_name, route_long_name, Service, direction_id, arrival_time, stop_sequence) %>% arrange(., route_short_name, Service, direction_id, arrival_time) %>% mutate(., Freq = difftime(arrival_time,lag(arrival_time),units='mins')) %>% na.omit() %>% filter(Freq>0)  %>% ungroup() %>% select(route_short_name, route_long_name, Service, Freq) %>% group_by(Service)

freq.mean <- bus.freq %>% summarise(Freq=mean(Freq))

most.freq <- bus.freq %>% filter(Freq==min(Freq)) %>% distinct() %>% mutate(Classification='Most Fequent') %>% ungroup()
leastfreq <- bus.freq %>% filter(Freq==max(Freq)) %>% distinct() %>% mutate(Classification='Least Fequent') %>% ungroup()

extreme.freq <- rbind(most.freq, leastfreq) %>% select(Service, Classification, Freq, Route=route_short_name, Route.name=route_long_name) %>% arrange(., Service, Freq) 
```

The mean frequencies across all routes for each type of service are as follows: `r slice(freq.mean,1)$Service`: `r slice(freq.mean,1)$Freq` mins, `r slice(freq.mean,2)$Service`: `r slice(freq.mean,2)$Freq` mins and `r slice(freq.mean,3)$Service`: `r slice(freq.mean,3)$Freq` mins. The routes with the highest and lowest frequencies routes for each type of service, along with their frequency in minutes are shown in the table below.

```{r}
kable(extreme.freq)
```

```{r}
##trying to figure out first and last bus - getting bogged down
#bus.join %>% mutate(Service=case_when(service_id == 2 ~ 'Sunday/Bank Holiday', service_id == 1 ~ 'Weekday',  service_id == 3 #~ 'Saturday')) %>% select(Service, route_short_name, route_long_name, arrival_time) %>% #mutate(actual.time=case_when(arrival_time < as.hms('04:00:00') ~ arrival_time+as.hms('24:00:00')))  %>% #filter(route_short_name == 41, Service=='Sunday/Bank Holiday') %>% arrange(arrival_time)

#library(hms)
#typeof(as.hms('04:00:00'))

#%>% group_by(Service) %>% filter(arrival_time==min(arrival_time, na.rm=TRUE) | arrival_time==max(arrival_time, na.rm=TRUE)) %>% select(Service, route_short_name, route_long_name, arrival_time)

#bus.join %>% mutate(Service=case_when(service_id == 2 ~ 'Sunday/Bank Holiday', service_id == 1 ~ 'Weekday',  service_id == 3 ~ 'Saturday'))  %>% filter(route_short_name == 41) %>% select(Service, route_short_name, service_id, arrival_time, departure_time)  %>% distinct() %>% arrange(arrival_time)
```


The route map is shown below in Figure X, overlaid over a map of Dublin. The longest and shortest routes are shown.
```{r fig.height=14, fig.width=14}
lat.long.all <- select(stops.df, Latitude=stop_lat, Longitude=stop_lon) %>% mutate(., Route='All Routes') %>% select(Route, Longitude, Latitude) %>% distinct()

lat.long.longest <- filter(bus.join, route_short_name==filter(route.length, Dist.mean==max(Dist.mean))$route_short_name) %>% select(., Route=route_short_name, Longitude=stop_lon, Latitude=stop_lat) %>% distinct()

lat.long.shortest <- filter(bus.join, route_short_name==filter(route.length, Dist.mean==min(Dist.mean))$route_short_name) %>% select(., Route=route_short_name, Longitude=stop_lon, Latitude=stop_lat) %>% distinct()

lat.long.combo <- rbind(lat.long.all, lat.long.longest, lat.long.shortest) 

# store bounding box coordinates - add a small amount so box extends slightly beyond the furthermost stop
dublin_bb <- c(left = (min(stops.df$stop_lon) - 0.01),
            bottom = (min(stops.df$stop_lat) - 0.01),
            right = (max(stops.df$stop_lon) + 0.01),
            top = (max(stops.df$stop_lat) + 0.01))

dublin_stamen <- get_stamenmap(bbox = dublin_bb, zoom = 11,  maptype = 'toner-lite')

ggmap(dublin_stamen, legend = 'right') + geom_point(data = lat.long.combo, mapping = aes(x = Longitude, y = Latitude, na.rm=TRUE, colour = Route, size=Route, alpha=Route)) +ggtitle('Figure X: Map of Dublin Bus Routes') + labs(x='Longitude', y='Latitude') + scale_colour_manual(values = c("red", "springgreen3", "deepskyblue3")) + scale_size_manual(values = c(2, 1.8, 0.7)) + scale_alpha_manual(values = c(1, 1, 0.5))
```

The units of distance travelled are not specified in the dataset. The distance on the 16 route from the George's Street stop to the Redmond's Hill stop is given in the dataset is `r dist.calc`. The distance between these two stops according to Google maps is 500m, so it appears that the unit of distance is meters. The mean and median route length are very similar at `r mean(route.length$Dist.mean)` km and `r median(route.length$Dist.mean)` km respectively. The shortest route is Route `r filter(route.length, Dist.mean==min(Dist.mean))$route_short_name`, `r filter(route.length, Dist.mean==min(Dist.mean))$route_long_name`, at `r filter(route.length, Dist.mean==min(Dist.mean))$Dist.mean` km. The longest route is Route `r filter(route.length, Dist.mean==max(Dist.mean))$route_short_name`, `r filter(route.length, Dist.mean==max(Dist.mean))$route_long_name`, at `r filter(route.length, Dist.mean==max(Dist.mean))$Dist.mean` km. Both can ben seen in Figure X above.

### Pick a route
```{r}
home.route <- 49
bus.home <- bus.join %>% filter(route_short_name==home.route, Service=='Weekday')
```

I live in Harold's Cross and work in College Green so am picking Route `r home.route`, `r bus.home %>% select(route_long_name) %>% distinct()`. According to the data, direction_id = 1 is in the direction of `r bus.home %>% filter(direction_id==1) %>% select(stop_headsign) %>% distinct() `, so that's the direction I would get to work. Direction_id = 0 is in the direction of `r bus.home %>% filter(direction_id==0) %>% select(stop_headsign) %>% distinct()`, so that would be the direction I would get home from work. The stops in question are listed below.
```{r}
home.stops <- bus.home %>%  filter(stop_id=='8220DB001296' | stop_id == '8220DB004521' | stop_id=='8220DB001339' | stop_id=='8220DB005192')
kable(home.stops %>% select(route_short_name, route_long_name, direction_id, stop_headsign, stop_id, stop_name) %>% distinct() )
```

```{r}
commute.trips <- home.stops %>% filter((direction_id==1 & stop_id=='8220DB005192' & arrival_time>as.hms('08:15:00') & arrival_time<as.hms('09:00:00')) | (direction_id==0 & stop_id=='8220DB004521' & arrival_time>as.hms('17:00:00') & arrival_time<as.hms('17:45:00'))) %>% select(trip_id)

length.route <- home.stops %>% inner_join(commute.trips, by='trip_id') %>% arrange(trip_id, stop_sequence) %>% group_by(direction_id, trip_id) %>% mutate(length.time=(lead(arrival_time) - arrival_time)/60) %>% mutate(length.dist=(lead(shape_dist_traveled) - shape_dist_traveled)/1000) %>% ungroup() %>% select(direction_id, stop_headsign, length.time, length.dist) %>% filter(!is.na(length.dist)) %>% distinct()
```

The trip into work should take `r filter(length.route, direction_id==1)$length.time` mins and `r filter(length.route, direction_id==1)$length.dist` km. The trip home from work should take `r filter(length.route, direction_id==0)$length.time` mins and `r filter(length.route, direction_id==0)$length.dist` km. Neither of these times appear to take into consideration traffic though.

If I needed to arrive for work at between 8.15 and 9am, and want to get a bus home from work between 5pm and 5.45 pm, I'll have to get one of the services below. 

```{r}
kable(inner_join(home.stops, commute.trips, by='trip_id') %>% mutate(Journey=case_when(stop_headsign=='Pearse St' ~ 'Commute to work', stop_headsign=='The Square' ~ 'Commute to home') ) %>% select(Service, Journey, Direction=stop_headsign, stop_name, arrival_time))
```

The times for the earliest and latest busses on Route `r home.route` are below.
```{r}
kable(bus.join %>% group_by(Service, stop_headsign) %>% filter(!is.na(arrival_time), route_short_name==home.route) %>% summarise(earliest.bus=min(arrival_time), latest.bus= max(arrival_time)))
```



I like exploring Dublin on a Saturday but I don't have a car and I don't like walking between stops to change bus. Here I'll look at everywhere that I can get to, using busses that interconnect with my chosen route `r home.route`. This analysis can easily be changed to look at other routes by changing the bus route chosen above.
```{r}
bus.join.sat <- filter(bus.join, Service=='Saturday')
stops.home <- bus.join.sat %>% filter(route_short_name==home.route)

stops.intersect <- inner_join(stops.home, bus.join.sat, by='stop_id', suffix=c('.home', '.intersect')) %>% select(route_short_name.intersect) %>% distinct() %>% inner_join(bus.join.sat, by = c("route_short_name.intersect" = "route_short_name")) %>% select(Longitude=stop_lon, Latitude=stop_lat) %>% distinct()

intersect.routes <- rbind((stops.intersect %>% mutate(Route=paste('All Routes Intersecting with Route', home.route))), (stops.home %>% select(Longitude=stop_lon, Latitude=stop_lat) %>% mutate(Route=paste('Route', home.route)))) %>% select(Route, Longitude, Latitude)
```

```{r fig.height=14, fig.width=14}
# store bounding box coordinates - add a small amount so box extends slightly beyond the furthermost stop
dublin_intersect_bb <- c(left = (min(stops.intersect$Longitude) - 0.01),
            bottom = (min(stops.intersect$Latitude) - 0.01),
            right = (max(stops.intersect$Longitude) + 0.01),
            top = (max(stops.intersect$Latitude) + 0.01))

dublin_intersect_stamen <- get_stamenmap(bbox = dublin_intersect_bb, zoom = 11,  maptype = 'toner-lite')

ggmap(dublin_intersect_stamen, legend = 'right') + geom_point(data = intersect.routes, mapping = aes(x = Longitude, y = Latitude, na.rm=TRUE, colour = Route, size=Route, alpha=Route)) +ggtitle('Figure X: Map of Dublin Bus Routes') + labs(x='Longitude', y='Latitude') + scale_colour_manual(values = c("deepskyblue3", "purple")) + scale_size_manual(values = c(1, 1)) + scale_alpha_manual(values = c(0.5, 1))
```



# Task 4
```{r}
#loading in the footfall data
url.footfall <- 'https://data.smartdublin.ie/dataset/8204be0a-6348-459e-96e9-65bb75600ec3/resource/384fe47a-2f25-4f52-8fc5-8e61899951e9/download/pedestrianfootfall2013.ods'
download.file(url.footfall, 'footfall_ods.ods')
sheet.names <- ods_sheets('pedestrianfootfall2013.ods')
```

```{r}
#getting list of camera locations
camera.locations <- read_ods('pedestrianfootfall2013.ods',sheet.names[1])[1]
colnames(camera.locations) <- c('Loc') 
#camera.locations %>% filter(., grepl("Entrance Name:", Loc, fixed = TRUE)) 
```


```{r}
# reading in data from the selected cameras 
camera1.name <- 'Entrance Name: Capel St at Mullen'
camera2.name <- 'Entrance Name: Grafton St at M&S'

camera1.all <- data.frame(matrix(ncol = 15, nrow = 0))
colnames(camera1.all) <- c('Time', 'Mon.In', 'Mon.Out', 'Tue.In', 'Tue.Out', 'Wed.In', 'Wed.Out', 'Thurs.In', 'Thurs.Out', 'Fri.In', 'Fri.Out', 'Sat.In', 'Sat.Out', 'Sun.In', 'Sun.Out')

camera2.all <- data.frame(matrix(ncol = 15, nrow = 0))
colnames(camera2.all) <- c('Time', 'Mon.In', 'Mon.Out', 'Tue.In', 'Tue.Out', 'Wed.In', 'Wed.Out', 'Thurs.In', 'Thurs.Out', 'Fri.In', 'Fri.Out', 'Sat.In', 'Sat.Out', 'Sun.In', 'Sun.Out')

for(week.i in 1:length(sheet.names))
{
  f1 <- read_ods('pedestrianfootfall2013.ods',sheet.names[week.i])
  
  if (ncol(f1)==15)
  {
    colnames(f1) <- c('Time', 'Mon.In', 'Mon.Out', 'Tue.In', 'Tue.Out', 'Wed.In', 'Wed.Out', 'Thurs.In', 'Thurs.Out', 'Fri.In', 'Fri.Out', 'Sat.In', 'Sat.Out', 'Sun.In', 'Sun.Out')
  } else {
    colnames(f1) <- c('Time', 'Mon.In', 'Mon.Out', 'Tue.In', 'Tue.Out', 'Wed.In', 'Wed.Out', 'Thurs.In', 'Thurs.Out', 'Fri.In', 'Fri.Out', 'Sat.In', 'Sat.Out', 'Sun.In', 'Sun.Out', 'unknown1', 'unknown2', 'unknown3')
  }
  f1 <- select(f1, Time, Mon.In, Mon.Out, Tue.In, Tue.Out, Wed.In, Wed.Out, Thurs.In, Thurs.Out, Fri.In, Fri.Out, Sat.In, Sat.Out, Sun.In, Sun.Out)
  
  camera1.row <- which(grepl(camera1.name, f1$Time))
  camera2.row <- which(grepl(camera2.name, f1$Time))

  camera1.slice <- slice(f1, (camera1.row+3):(camera1.row+26)) %>% mutate(Week=sheet.names[week.i])
  camera2.slice <- slice(f1, (camera2.row+3):(camera2.row+26)) %>% mutate(Week=sheet.names[week.i])
  
  camera1.all <- rbind(camera1.all, camera1.slice)
  camera2.all <- rbind(camera2.all, camera2.slice)
  
  f1 <- NULL
}
```

```{r}
camera1.all
camera2.all

#all fields are character (they contained some text before using split), so must change all numeric characters to numeric
camera1.all$Mon.In <- as.numeric(camera1.all$Mon.In)
camera1.all$Mon.Out <- as.numeric(camera1.all$Mon.Out)
camera1.all$Tue.In <- as.numeric(camera1.all$Tue.In)
camera1.all$Tue.Out <- as.numeric(camera1.all$Tue.Out)
camera1.all$Wed.In <- as.numeric(camera1.all$Wed.In)
camera1.all$Wed.Out <- as.numeric(camera1.all$Wed.Out)
camera1.all$Thurs.In <- as.numeric(camera1.all$Thurs.In)
camera1.all$Thurs.Out <- as.numeric(camera1.all$Thurs.Out)
camera1.all$Fri.In <- as.numeric(camera1.all$Fri.In)
camera1.all$Fri.Out <- as.numeric(camera1.all$Fri.Out)
camera1.all$Sat.In <- as.numeric(camera1.all$Sat.In)
camera1.all$Sat.Out <- as.numeric(camera1.all$Sat.Out)
camera1.all$Sun.In <- as.numeric(camera1.all$Sun.In)
camera1.all$Sun.Out <- as.numeric(camera1.all$Sun.Out)

camera2.all$Mon.In <- as.numeric(camera2.all$Mon.In)
camera2.all$Mon.Out <- as.numeric(camera2.all$Mon.Out)
camera2.all$Tue.In <- as.numeric(camera2.all$Tue.In)
camera2.all$Tue.Out <- as.numeric(camera2.all$Tue.Out)
camera2.all$Wed.In <- as.numeric(camera2.all$Wed.In)
camera2.all$Wed.Out <- as.numeric(camera2.all$Wed.Out)
camera2.all$Thurs.In <- as.numeric(camera2.all$Thurs.In)
camera2.all$Thurs.Out <- as.numeric(camera2.all$Thurs.Out)
camera2.all$Fri.In <- as.numeric(camera2.all$Fri.In)
camera2.all$Fri.Out <- as.numeric(camera2.all$Fri.Out)
camera2.all$Sat.In <- as.numeric(camera2.all$Sat.In)
camera2.all$Sat.Out <- as.numeric(camera2.all$Sat.Out)
camera2.all$Sun.In <- as.numeric(camera2.all$Sun.In)
camera2.all$Sun.Out <- as.numeric(camera2.all$Sun.Out)

# there was inconsistency in sheet naming so fixing this
camera1.all$Week <- gsub('Sheet13', 'Week_13', camera1.all$Week)
camera2.all$Week <- gsub('Sheet13', 'Week_13', camera2.all$Week)

camera1.all$Week <- gsub('Week_1\\>', 'Week_01', camera1.all$Week)
camera1.all$Week <- gsub('Week_2\\>', 'Week_02', camera1.all$Week)
camera1.all$Week <- gsub('Week_3\\>', 'Week_03', camera1.all$Week)
camera1.all$Week <- gsub('Week_4\\>', 'Week_04', camera1.all$Week)
camera1.all$Week <- gsub('Week_5\\>', 'Week_05', camera1.all$Week)
camera1.all$Week <- gsub('Week_6\\>', 'Week_06', camera1.all$Week)
camera1.all$Week <- gsub('Week_7\\>', 'Week_07', camera1.all$Week)
camera1.all$Week <- gsub('Week_8\\>', 'Week_08', camera1.all$Week)
camera1.all$Week <- gsub('Week_9\\>', 'Week_09', camera1.all$Week)
camera2.all$Week <- gsub('Week_1\\>', 'Week_01', camera2.all$Week)
camera2.all$Week <- gsub('Week_2\\>', 'Week_02', camera2.all$Week)
camera2.all$Week <- gsub('Week_3\\>', 'Week_03', camera2.all$Week)
camera2.all$Week <- gsub('Week_4\\>', 'Week_04', camera2.all$Week)
camera2.all$Week <- gsub('Week_5\\>', 'Week_05', camera2.all$Week)
camera2.all$Week <- gsub('Week_6\\>', 'Week_06', camera2.all$Week)
camera2.all$Week <- gsub('Week_7\\>', 'Week_07', camera2.all$Week)
camera2.all$Week <- gsub('Week_8\\>', 'Week_08', camera2.all$Week)
camera2.all$Week <- gsub('Week_9\\>', 'Week_09', camera2.all$Week)
```

```{r}
camera1.all <- camera1.all %>% mutate(Mon=Mon.In+Mon.Out) %>% mutate(Tue=Tue.In+Tue.Out) %>% mutate(Wed=Wed.In+Wed.Out) %>% mutate(Thurs=Thurs.In+Thurs.Out) %>% mutate(Fri=Fri.In+Fri.Out) %>% mutate(Sat=Sat.In+Sat.Out) %>% mutate(Sun=Sun.In+Sun.Out) %>% mutate(All=Mon+Tue+Wed+Thurs+Fri+Sat+Sun) %>% mutate(Week.fact=as.factor(Week))

camera2.all <- camera2.all %>% mutate(Mon=Mon.In+Mon.Out) %>% mutate(Tue=Tue.In+Tue.Out) %>% mutate(Wed=Wed.In+Wed.Out) %>% mutate(Thurs=Thurs.In+Thurs.Out) %>% mutate(Fri=Fri.In+Fri.Out) %>% mutate(Sat=Sat.In+Sat.Out) %>% mutate(Sun=Sun.In+Sun.Out) %>% mutate(All=Mon+Tue+Wed+Thurs+Fri+Sat+Sun) %>% mutate(Week.fact=as.factor(Week))

camera1.all
camera2.all
```
```{r fig.height=14, fig.width=14}
camera1.sum.week <- camera1.all %>% group_by(Week.fact) %>% summarise(Footfall=sum(All)) %>% mutate(Camera = gsub("Entrance Name: ","", camera1.name))
camera2.sum.week <- camera2.all %>% group_by(Week.fact) %>% summarise(Footfall=sum(All)) %>% mutate(Camera = gsub("Entrance Name: ","", camera2.name))
```

The footfall in Capel Street (average per week over 2013 is `r mean(camera1.sum.week$Footfall)`) tends to be much lower than that in Grafton street (average per week over 2013 is `r mean(camera2.sum.week$Footfall)`), which would be expected as Grafton is a very busy shopping street and Capel Street would be expected to have less ttraffic. We can see the amoutnt of footfall over the course of the year in Figure X below.

```{r fig.height=14, fig.width=14}
ggplot(data=rbind(camera1.sum.week, camera2.sum.week), aes(x=Week.fact, y=Footfall, group=Camera, colour = Camera)) + geom_point() + geom_line() + facet_grid(rows=vars(Camera), scales='free_y') + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = 'Week of Year 2013', y = 'Footfall', title='Footfall for weeks of year 2013')
```








### Task 2.2
I chose an ambient sound dataset
```{r}
ambient.sound.url <- 'https://data.smartdublin.ie/dataset/a52fbbe2-1bff-4897-84af-34945f6fc8de/resource/30cb9275-f9fb-432a-af48-0a5f7ab462a4/download/chancerypark2013.zip'
download.file(ambient.sound.url, destfile = 'ambient_sound.zip')
unzip('ambient_sound.zip', exdir = './ambient_sound')
```

```{r}
# I need to loop through all dates in the year to pull out each text file. Here I create a df of all required components
months <- format(ISOdate(2013,1:12,1),"%B")

dates.df <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(dates.df) <- c('year', 'month.char', 'month.num', 'day')

for (i in 1:12)
{
  date.i <- data.frame(matrix(ncol = 4, nrow = 1))
  colnames(date.i) <- c('year', 'month.char', 'month.num', 'day')
  
  date.i$year <- '2013'
  date.i$month.char <- months[i]
  if(nchar(as.character(i))==1)
  {
    date.i$month.num <- paste('0', i, sep='')
  } else {
    date.i$month.num <- i
  }
  if(months[i]=='December')
  {
   date.i$day = 28 #the last 3 days of the year are excluded from the footfall dataset
  } else {
  date.i$day <- as.Date(paste('2013','-', i ,'-01', sep=''), '%Y-%m-%d') %>% days_in_month()
  }
  dates.df <- rbind(dates.df, date.i)
  
  date.i <- NULL
}

```

```{r}
sound.all <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(sound.all) <- c('week', 'day.week', 'Time', 'mean.noise.level')

for (i.month in 1:nrow(dates.df))
{
  for (i.day in 1:dates.df$day[i.month])
  {
    
    if(nchar(as.character(i.day))==1)
     {
      sound.file <- paste('ambient_sound/',dates.df$year[i.month], '/', dates.df$month.char[i.month], '/DCC-NOISE-001011-2013-', dates.df$month.num[i.month], '-0', i.day, 'T00-00-00.txt', sep='')
    } else {
     sound.file <- paste('ambient_sound/2013/', dates.df$month.char[i.month], '/DCC-NOISE-001011-2013-', dates.df$month.num[i.month], '-', i.day, 'T00-00-00.txt', sep='')
    }
    if (!file.exists(sound.file)) next

    sound.read <- read.table(sound.file) 
    sound.df <- sound.read %>% select(date.sound=V1, time=V2, noise.level=V3) 

    sound.all <- rbind.data.frame(sound.all, sound.df)
    
    sound.df<- NULL
  }
}
```


```{r}
sound.tidy <- sound.all
sound.tidy$time <-  gsub(",","", sound.tidy$time)
sound.tidy$noise.level <-  as.numeric(gsub(",","", sound.tidy$noise.level))
sound.tidy$date.sound <-  as.Date(sound.tidy$date.sound, format='%d/%m/%Y')

sound.tidy$Time <- format(strptime(sound.tidy$time,format='%H:%M:%S'), "%k")
sound.tidy.summary <- sound.tidy %>% group_by(date.sound, Time) %>% summarise(mean.noise.level=mean(noise.level)) 

sound.tidy.summary$Week <- paste('Week', strftime(sound.tidy.summary$date.sound, "%V"), sep='_')
sound.tidy.summary$day.week <- format(as.Date(sound.tidy.summary$date.sound), "%A") 

sound.tidy.all <- sound.tidy.summary[2:5] 
```


```{r}
sound.split <- split(sound.tidy.all, sound.tidy.all$day.week)

sound.mon <- sound.split$Monday %>% select(., Week, Time, mon.noise=mean.noise.level)
sound.tue <- sound.split$Tuesday %>% select(Week, Time, tue.noise=mean.noise.level)
sound.wed <- sound.split$Wednesday %>% select(Week, Time, wed.noise=mean.noise.level)
sound.thurs <- sound.split$Thursday %>% select(Week, Time, thurs.noise=mean.noise.level)
sound.fri <- sound.split$Friday %>% select(Week, Time, fri.noise=mean.noise.level)
sound.sat <- sound.split$Saturday %>% select(Week, Time, sat.noise=mean.noise.level)
sound.sun <- sound.split$Sunday %>% select(Week, Time, sun.noise=mean.noise.level)

sound.new <- sound.mon %>% full_join(sound.tue, by=c('Week', 'Time')) %>% full_join(sound.wed, by=c('Week', 'Time')) %>% full_join(sound.thurs, by=c('Week', 'Time')) %>% full_join(sound.fri, by=c('Week', 'Time')) %>% full_join(sound.sat, by=c('Week', 'Time')) %>% full_join(sound.sun, by=c('Week', 'Time')) %>% arrange(Week, Time)  %>% mutate(Week.fact=as.factor(Week))

sound.new[is.na(sound.new)] <- 0

sound.all <- sound.new %>% mutate(All=mon.noise+tue.noise+wed.noise+thurs.noise+fri.noise+sat.noise+sun.noise)
sound.all
```


```{r fig.height=14, fig.width=14}
sound.week <- sound.all %>% group_by(Week.fact) %>% summarise(Sound_Level=sum(All))
sound.week
camera1.sum.week


ggplot(data=camera1.sum.week, aes(x=Week.fact, y=Footfall, group=1)) + geom_point() + geom_line() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = 'Routes', y = 'Number of Trips', title='Figure X: Number of journeys on each route by service type')

ggplot(data=sound.week, aes(x=Week.fact, y=Sound_Level, group=1)) + geom_point() + geom_line() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = 'Routes', y = 'Number of Trips', title='Figure X: Number of journeys on each route by service type')
```

```{r fig.height=14, fig.width=14}
camera1.all.w12  <- camera1.all %>% filter(Week=='Week_12') %>% select(Time, Mon) %>% mutate(Label='Footfall')
camera1.all.w12


sound.all.w12  <- sound.all %>% filter(Week=='Week_12') %>% select(Time, Mon=mon.noise) %>% mutate(Label='Ambient Noise')
camera1.all.w12
sound.all.w12

ggplot(data=rbind(camera1.all.w12, sound.all.w12), aes(x=Time, y=Mon, group=Label, colour = Label)) + geom_point() + geom_line() + facet_grid(rows=vars(Label), scales='free_y') + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = 'Routes', y = 'Number of Trips', title='Figure X: Number of journeys on each route by service type')
```



# 5 Reflection
## 5.1 Analytic
Here I analyse the the history of my log console for the duration of this assignment.
```{r}
#The history required spans multiple files so will add all here and remove duplicates and rows about other subjects
file.list <- list('history_database/history_database', 'history_database/history_database_20181114_end',  'history_database/history_database_20181105_end', 'history_database/history_database_20181103','history_database/history_database_20181031')

history.all <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(sound.all) <- c('date.mili', 'code')

for (i.hist in 1:length(file.list))
{
  history.read <- read.delim(file.list[[i.hist]], header=FALSE, sep=':', stringsAsFactors=FALSE) 
  history.all <- rbind.data.frame(history.all, history.read)
  
  history.read<- NULL
}

history <- history.all %>% distinct() %>% mutate(hist.date.time.base=as.POSIXct(as.numeric(V1)/1000, origin=as.Date('1970-01-01', "%Y-%m-%d"), format = "%Y-%m-%d %H:%M:%OS")) %>% mutate(hist.date=format(hist.date.time.base, format = "%Y-%m-%d")) %>% mutate(hist.time=format(hist.date.time.base, format = "%H")) %>% mutate(hist.date.time=format(hist.date.time.base, format = "%Y-%m-%d: %H")) %>% mutate(hist.date.time.min=format(hist.date.time.base, format = "%Y-%m-%d: %H:%M")) %>% mutate(hist.time=format(hist.date.time.base, format = "%H")) %>% filter(V1 > 1541359887621) %>% select(hist.date, hist.time, hist.date.time, hist.date.time.min, hist.text=V2) %>% filter(hist.date != as.Date('1970-01-01', "%Y-%m-%d"))

hist.simple <- gsub(",","", history$hist.text)
hist.simple <- gsub("'","", hist.simple)
word.freq <- data.frame(table(unlist(strsplit(tolower(hist.simple), " ")))) %>% arrange(desc(Freq))

#history[which(grepl('distinct()', history$hist.text)),]
#history[which(grepl('nrow', history$hist.text)),]

hist.count <- count(history, hist.date.time)
```

I ran a total of `r nrow(history)` lines of code, which contained `r sum(stri_count_words(history$hist.text))` words. I ran on average, `r summarise(hist.count, mean(n))` commands an hour, or `r summarise(hist.count, mean(n))/60` per minute, with a median of `r summarise(hist.count, median(n))/60` commands per minute and a maximum of `r summarise(hist.count, max(n))/60` per minute. As I was running a lot of for loops, this high rate isn't unexpected.

The most frequently run commands involved, as expected, assignment and piping:, <- at `r word.freq %>% filter(Var1=='<-') %>% select(Freq)` times, %>% at `r word.freq %>% filter(Var1=='%>%') %>% select(Freq)` times and = at `r word.freq %>% filter(Var1=='=') %>% select(Freq)` times. The most common R functions were distinct() at `r word.freq %>% filter(Var1=='distinct()') %>% select(Freq)` and nrow at `r word.freq %>% filter(Var1=='nrow') %>% select(Freq)`, which I found a bit surprising until I realised that I use nrow in the footfall loop to read in the data and I used distinct() in the bus route data, which I had a bit of trouble with. The most common variables were the column names I gave to the footfall columns such as mon.in, mon.out etc, at`r word.freq %>% filter(Var1=='fri.in') %>% select(Freq)` each. As these run inside a loop to read in all the footfall data, it's not surprising that these were the most common.

As I've been trying to get better at using the built in R help, I measured the number of times I used the help function by looking at the number of times '?' was used. I probably still have to get better at using it, as you can see from the figure below.

```{r}
hist.count.all <- history %>% count(hist.date.time) %>% mutate(label='All Commands')
hist.help <- history[which(grepl('\\?', history$hist.text)),] %>% count(hist.date.time) %>% mutate(label='Help Function Used')

ggplot(data=rbind(hist.help, hist.count.all), aes(x=hist.date.time, y=n)) + facet_grid(rows=vars(label), scale = 'free_y') + geom_histogram(stat = 'identity', fill='darkorchid3', colour='black') + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(x = 'Date and Hour', y = 'Number of Commands Run', title='Figure X: Number of Commands Run per Date and Hour')
```

I can see from the graph that I tended to work in large chunks, not taking many breaks. This isn't ideal for concentration, but as I have other commitments, it's the only way I can fit in the work.


## 5.2 Personal Reflection
1.)
challenges overcome (work-arounds), 2.) efficient work practices 3.) areas of
frustration 4.) time management. A useful way to structure a reflection is by
using the four I's e.g. Identify an element during the project (past) - results,
quality, emotions, productivity. Investigate - determine the source of this (present).
Incorporate - based on the investigation, is there a lesson to absorb (future)?
Improve - the goal of reflection.

The length of the project was the main challenge for me. Some of the parts were quite complex but I enjoyed figuring them out, my main issue was finding time to do everything.

I decided to approach the project by attempting each problem until I got stuck and then moving on to the next one if I was stuck for more than 20 minutes or so, and then going back to the start to finish the problems to the required standard. I've learned from previous experience that it's easier for me to work on improving a question rather than starting something from scratch. By looking at each question in some detail at the start, is also allows ideas to perculate, so when you come back to it later, you might ahve solved the problem or have a new perspective. When I actually stuck to this, it worked well, but a few times I kept at something when I just should have given up and moved on, at least for the moment. Towards the end of the assignment, I made a Trello to do list whch had the 'minimum viable answer' for each task, so I would stick to doing that. I ca go back and improve it later.

I found some of the tasks challenging, like finding a dataset to compare to the footfall one, mainly because the footfall data was from 2013 and it was difficult to find an easily accessible dataset from that time.

 

It's also easy to get stuck down a rabbit hole of trying to solve a particular technical problem but then when you looka t it with a fresh mind, you realised that problem didn't need to be solved at all for the question.

Got stuck down a rabbit hole of joining together dataframes in task 3 and got stuck.
Came back to it another day and reread the question and realised I was making it overly complicated.

I sometimes found it hard to figure out whether there was a small issue with my code and I was almost there, or if I was completely wrong: loading bike JSON.

As some of the questions were of the type 'find the most interesting information', I decided to first so something relatively basic and straightforward and then come back to it at a later date and improve it if I had time. This would also hopefully give me time to come up with a better idea.

BEcause we have a test coming up where we won't have internet access, I've started using the built in R help function instead of googling the same information, whichi I think it a better habit to get into as you get to understand the inner workings of the function instead of letting someone else intrepret it for you.


[1] https://www.irishexaminer.com/ireland/fivefold-increase-in-online-shopping-since-2007-469059.html (original Euromonitor International report not freely available online)

[2] https://www.cso.ie/px/pxeirestat/Database/eirestat/Overseas%20Travel/Overseas%20Travel_statbank.asp?SP=Overseas%20Travel&Planguage=0y (report TMA14: Overseas Trips to and from Ireland by Trips and Year)

[1] https://developers.google.com/transit/gtfs/reference/